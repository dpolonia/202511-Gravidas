# Gravidas Pipeline v1.2.0 - Development Roadmap

**Version:** 1.2.0
**Target Release:** March 2026
**Focus:** Technical Excellence + Honest Research Framing
**Strategic Path:** Operations/Cost Optimization (Path B)

---

## Executive Summary

Version 1.2.0 represents a strategic pivot from overclaiming clinical validation to delivering honest, high-quality operations research on multi-provider AI cost-optimization for synthetic healthcare data generation.

**Core Principle:** Build what we can validate, claim what we can prove.

---

## Critical Reframing (MANDATORY)

### What v1.1.0 Implied (PROBLEMATIC):
- ❌ "Prove synthetic interviews can replace real human interviews"
- ❌ "Obtain the same results" as real interviews
- ❌ Clinical validation of synthetic data equivalence

### What v1.2.0 Will Deliver (HONEST):
- ✅ "Cost-optimized framework for large-scale synthetic healthcare interview generation"
- ✅ "89% cost reduction through multi-provider AI orchestration"
- ✅ "Operations research contribution to AI service procurement"
- ✅ "Training data generation for healthcare NLP applications"

**Key Change:** Remove ALL implications that synthetic interviews are validated replacements for real human research.

---

## Phase 1: Critical Technical Fixes (Weeks 1-4)

### 1.1 Fix Semantic Tree Generation (BLOCKER)
**Priority:** P0 (Critical Blocker)
**Current State:** 90% failure rate (9/10 health records fail)
**Target State:** <5% failure rate (95/100 success)
**Effort:** 40 hours

**Tasks:**
- [ ] Debug NoneType error in `scripts/utils/semantic_tree_builder.py`
- [ ] Add robust null-checking for FHIR fields
- [ ] Implement fallback logic when required fields missing
- [ ] Add comprehensive error logging
- [ ] Test on all 665 existing health records
- [ ] Validate semantic matching algorithm with working trees

**Acceptance Criteria:**
- Semantic tree generation succeeds for ≥95% of health records
- Matching algorithm leverages full 60% semantic weighting
- Error messages clearly identify missing FHIR fields

**Deliverable:** `scripts/utils/semantic_tree_builder_v2.py` with ≥95% success rate

---

### 1.2 Complete FHIR Data Extraction (HIGH)
**Priority:** P0 (Critical for Realism)
**Current State:** 50% missing pregnancy weeks, 100% missing vitals
**Target State:** ≥90% data completeness
**Effort:** 30 hours

**Tasks:**
- [ ] Extract pregnancy weeks from FHIR Observation resources
- [ ] Parse blood pressure from vitals (systolic/diastolic)
- [ ] Extract fetal heart rate from pregnancy observations
- [ ] Add weight gain tracking from measurements
- [ ] Implement gestational age calculation from pregnancy start date
- [ ] Add maternal age at conception
- [ ] Handle missing data gracefully (mark as "Unknown" vs. error)

**Acceptance Criteria:**
- ≥90% of interviews have pregnancy weeks populated
- ≥80% have blood pressure data
- ≥70% have fetal heart rate (if clinically appropriate for stage)
- Clear documentation of which FHIR resources are parsed

**Deliverable:** Enhanced `scripts/utils/fhir_parser.py` with comprehensive vital signs extraction

---

### 1.3 Calibrate Anomaly Detection (MEDIUM)
**Priority:** P1 (Quality Assurance)
**Current State:** 100% false positive rate
**Target State:** <10% false positive rate on normal data
**Effort:** 20 hours

**Tasks:**
- [ ] Generate baseline metrics from 100-interview dataset
- [ ] Calculate mean + 2-3 standard deviations for thresholds
- [ ] Implement dynamic threshold adjustment based on dataset size
- [ ] Add percentile-based outlier detection (99th percentile)
- [ ] Create separate thresholds for different interview protocols
- [ ] Add flagging severity levels (warning vs. critical)

**Acceptance Criteria:**
- Run on 100 diverse interviews
- Flag ≤10% as anomalies (not 100%)
- True anomalies (manually identified) detected at >90% rate
- Clear explanation of why each interview is flagged

**Deliverable:** Recalibrated `scripts/05_analyze_interviews.py` with validated thresholds

---

### 1.4 Add Automated Testing (MEDIUM-HIGH)
**Priority:** P1 (Code Quality)
**Current State:** Zero automated tests
**Target State:** ≥60% code coverage with pytest
**Effort:** 25 hours

**Tasks:**
- [ ] Create `tests/` directory structure
- [ ] Add unit tests for semantic tree builder
- [ ] Add unit tests for FHIR parser
- [ ] Add unit tests for matching algorithm
- [ ] Add integration tests for full pipeline
- [ ] Add fixture data for reproducible testing
- [ ] Implement CI/CD with GitHub Actions (optional)

**Acceptance Criteria:**
- ≥60% code coverage on core modules
- All tests pass on Python 3.11+
- Tests run in <2 minutes
- Documented test strategy in `tests/README.md`

**Deliverable:** `tests/` directory with comprehensive test suite

---

## Phase 2: Enhanced Capabilities (Weeks 5-8)

### 2.1 Multiple Interview Protocols (HIGH)
**Priority:** P1 (Research Versatility)
**Current State:** 1 protocol (prenatal_care.json)
**Target State:** 4-5 diverse protocols
**Effort:** 30 hours

**Tasks:**
- [ ] **Protocol 2:** Postpartum care interview (15 questions)
- [ ] **Protocol 3:** High-risk pregnancy interview (18 questions)
- [ ] **Protocol 4:** Mental health screening (12 questions)
- [ ] **Protocol 5:** Genetic counseling interview (10 questions)
- [ ] Update interview script to handle multiple protocols
- [ ] Add protocol selection in interactive mode
- [ ] Validate each protocol with 5 test interviews

**Acceptance Criteria:**
- Each protocol covers distinct clinical scenario
- Questions appropriate for target population
- All protocols executable through pipeline
- Documentation for when to use each protocol

**Deliverable:** `Script/interview_protocols/` with 5 validated protocols

---

### 2.2 Persona Name Extraction (LOW)
**Priority:** P2 (User Experience)
**Current State:** All interviews show "Unknown" persona name
**Target State:** 90%+ interviews have names
**Effort:** 8 hours

**Tasks:**
- [ ] Parse first name from persona description field
- [ ] Add name generation fallback if description missing
- [ ] Populate `persona_name` field in output CSV
- [ ] Update interview transcripts to use actual names

**Acceptance Criteria:**
- ≥90% of interviews have persona names
- Names culturally appropriate for persona demographics
- Graceful fallback for missing names

**Deliverable:** Enhanced persona parser with name extraction

---

### 2.3 Enhanced Cost Tracking (MEDIUM)
**Priority:** P2 (Financial Analysis)
**Current State:** Approximate token counting
**Target State:** Exact API cost tracking
**Effort:** 15 hours

**Tasks:**
- [ ] Capture exact token counts from API responses (not estimates)
- [ ] Log real-time costs during interview execution
- [ ] Add per-stage cost breakdown (persona gen, interviews, analysis)
- [ ] Create cost comparison dashboard/visualization
- [ ] Add budget forecasting with confidence intervals

**Acceptance Criteria:**
- Token counts match API billing exactly (±1%)
- Real-time cost tracking during execution
- Cost breakdown by stage and provider
- Forecasting accuracy within 5% for 100+ interviews

**Deliverable:** Enhanced cost tracking module with dashboard

---

## Phase 3: Documentation & Reproducibility (Weeks 9-10)

### 3.1 Architecture Documentation (MEDIUM)
**Priority:** P2 (Developer Onboarding)
**Effort:** 12 hours

**Tasks:**
- [ ] Create system architecture diagram (data flow)
- [ ] Document each pipeline stage with inputs/outputs
- [ ] Add API reference documentation (function signatures)
- [ ] Create developer onboarding guide
- [ ] Document configuration options comprehensively

**Deliverable:** `docs/ARCHITECTURE.md` + `docs/API_REFERENCE.md`

---

### 3.2 Ethical Use Guidelines (HIGH)
**Priority:** P1 (Responsible AI)
**Effort:** 10 hours

**Tasks:**
- [ ] Document appropriate use cases (training data, research)
- [ ] Document inappropriate use cases (presenting as real, clinical decisions)
- [ ] Add disclaimer about limitations
- [ ] Include bias acknowledgment (AI-generated personas may perpetuate biases)
- [ ] Add citation requirements for users

**Deliverable:** `docs/ETHICAL_USE.md` + README section

---

### 3.3 Research Manuscript Reframing (CRITICAL)
**Priority:** P0 (Publication Success)
**Effort:** 20 hours

**Tasks:**
- [ ] **NEW Title:** "Multi-Provider AI Cost-Optimization for Large-Scale Synthetic Healthcare Interview Generation: An Operations Research Framework"
- [ ] Rewrite abstract to focus on cost optimization (remove clinical claims)
- [ ] Emphasize contribution to operations management literature
- [ ] Add limitations section acknowledging no real interview comparison
- [ ] Strengthen cost analysis with sensitivity analysis
- [ ] Add decision framework for AI service procurement
- [ ] Position as "training data generation" not "replacement for real interviews"

**Acceptance Criteria:**
- Zero claims about replacing real interviews
- Clear focus on operations/cost optimization
- Honest acknowledgment of limitations
- Strong contribution to operations management field

**Deliverable:** Manuscript draft aligned with IJPE scope

---

## Phase 4: Final Validation & Launch (Weeks 11-12)

### 4.1 Large-Scale Testing (100 Interviews)
**Priority:** P1
**Effort:** 8 hours execution + 4 hours analysis

**Tasks:**
- [ ] Execute 100 interviews across all 5 protocols
- [ ] Validate anomaly detection on larger dataset
- [ ] Measure quality metrics (semantic matching, data completeness)
- [ ] Analyze cost variance across providers
- [ ] Document any pipeline failures

**Acceptance Criteria:**
- ≥95% interview completion success rate
- Anomaly detection flags <10% of interviews
- Cost per interview within expected range (±10%)
- All 5 protocols successfully executed

**Deliverable:** 100-interview validation report

---

### 4.2 Open-Source Release Preparation
**Priority:** P2
**Effort:** 10 hours

**Tasks:**
- [ ] Clean up repository (remove sensitive data)
- [ ] Add comprehensive CONTRIBUTING.md
- [ ] Create CHANGELOG.md documenting v1.0 → v1.2 changes
- [ ] Add issue templates for bug reports and feature requests
- [ ] Create release notes for v1.2.0
- [ ] Tag GitHub release

**Deliverable:** Polished open-source repository ready for community use

---

## Success Metrics for v1.2.0

### Technical Quality
- [ ] Semantic tree generation: ≥95% success rate (from 10%)
- [ ] FHIR data completeness: ≥90% (from ~50%)
- [ ] Anomaly detection false positive rate: <10% (from 100%)
- [ ] Test coverage: ≥60% (from 0%)
- [ ] Pipeline success rate: ≥95% (from 100% on 10 interviews, validate on 100)

### Research Quality
- [ ] Zero overclaims about clinical validation
- [ ] Honest framing as operations research
- [ ] Clear limitations documented
- [ ] Ethical use guidelines published
- [ ] Reproducibility validated by external user

### Operational Excellence
- [ ] 5 interview protocols available (from 1)
- [ ] Multi-provider cost comparison validated
- [ ] Exact cost tracking implemented
- [ ] Comprehensive documentation published

---

## Risk Management

### High-Risk Items
1. **Semantic tree generation fix** - May require deep debugging
   - **Mitigation:** Allocate 2-week buffer; have fallback to demographic-only matching

2. **FHIR data extraction** - Synthea output may not contain required fields
   - **Mitigation:** Document which fields are available vs. missing; set realistic expectations

3. **Manuscript reframing** - May face reviewer skepticism even with honest framing
   - **Mitigation:** Strong operations focus; leverage novel cost-optimization framework

### Medium-Risk Items
4. **Test coverage** - May be time-consuming to achieve 60%
   - **Mitigation:** Focus on critical path code first; accept lower coverage if needed

5. **Multiple protocols** - May not have domain expertise for all scenarios
   - **Mitigation:** Use ChatGPT/Claude to draft protocols; validate against published clinical guidelines

---

## Budget Allocation for v1.2.0 Development

**Total Available:** €5,000 (from v1.1.0 plan)

| Category | Amount | Purpose |
|----------|--------|---------|
| Development Time (if outsourced) | €0 | Assume internal development |
| AI Execution (100 test interviews) | €100 | Validation testing across 5 protocols |
| AI Execution (10,000 production) | €2,150 | Original plan maintained |
| Publication APC (IJPE) | €2,800 | Original plan maintained |
| Infrastructure (AWS) | €50 | Original plan maintained |
| **Buffer** | -€100 | Used for testing |
| **TOTAL** | **€5,000** | On budget |

---

## Timeline Summary

| Phase | Duration | Key Deliverables |
|-------|----------|------------------|
| **Phase 1: Critical Fixes** | Weeks 1-4 | Semantic trees fixed, FHIR complete, anomaly detection calibrated, tests added |
| **Phase 2: Enhanced Capabilities** | Weeks 5-8 | 5 protocols, cost tracking, name extraction |
| **Phase 3: Documentation** | Weeks 9-10 | Architecture docs, ethical guidelines, manuscript reframed |
| **Phase 4: Validation** | Weeks 11-12 | 100-interview test, open-source release |

**Total Duration:** 12 weeks (3 months) - Aligns with January-March 2026 plan

---

## v1.2.0 Release Criteria (Definition of Done)

Version 1.2.0 is ready for release when ALL of the following are true:

- [ ] Semantic tree generation success rate ≥95%
- [ ] FHIR data completeness ≥90%
- [ ] Anomaly detection false positive rate <10%
- [ ] Test coverage ≥60% on core modules
- [ ] 5 interview protocols available and tested
- [ ] Exact cost tracking implemented
- [ ] Architecture documentation complete
- [ ] Ethical use guidelines published
- [ ] Manuscript reframed with honest claims only
- [ ] 100-interview validation test passed (≥95% success)
- [ ] All v1.2.0 commits tagged in GitHub
- [ ] CHANGELOG.md updated
- [ ] README.md updated to reflect new capabilities

---

## Post-v1.2.0: Future Directions (v1.3.0+)

Once v1.2.0 is complete and published, consider these paths for future versions:

### Option 1: Add Limited Clinical Validation (v1.3.0)
- Collect 10-20 real interviews for comparison
- Healthcare professional blind review
- Publish validation study in health informatics journal
- **Investment:** €3,000-5,000 | **Timeline:** 4-6 months

### Option 2: Scale to Other Healthcare Domains (v1.3.0)
- Diabetes management interviews
- Cancer survivorship interviews
- Mental health counseling scenarios
- **Investment:** Minimal (protocol development) | **Timeline:** 2-3 months

### Option 3: Advanced NLP Analysis (v1.3.0)
- Topic modeling across 10,000 interviews
- Conversation flow analysis
- Sentiment evolution tracking
- Automated quality scoring
- **Investment:** €1,000-2,000 (if outsourced) | **Timeline:** 2-3 months

---

## Conclusion

Version 1.2.0 pivots the Gravidas pipeline from **overclaimed clinical validation** to **honest, high-quality operations research**.

**What we gain:**
- Scientific credibility through honest framing
- Technical excellence through critical bug fixes
- Research versatility through multiple protocols
- Publication success through realistic claims

**What we abandon:**
- Unsupported claims about replacing real interviews
- Clinical validation without real data
- Misleading research positioning

**The result:** A valuable, publishable contribution to operations management research on AI cost-optimization for synthetic healthcare data generation.

---

**Version:** 1.0
**Date:** 2025-11-15
**Status:** Proposed for approval
**Next Review:** Upon user approval → Begin Phase 1
