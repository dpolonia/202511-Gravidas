# Synthetic Gravidas Pipeline Configuration
# Enhanced with multiple model selection options

###############################################################################
# ACTIVE MODEL SELECTION
# ---------------------------------------------------------------------------
# Choose your provider and model below. Uncomment the ones you want to use.
###############################################################################

# ACTIVE PROVIDER: Choose one (anthropic, openai, or google)
active_provider: "anthropic"

# ACTIVE MODEL: Set the model name for your chosen provider
active_model: "claude-4.5-sonnet"  # Change this to any model listed below

###############################################################################
# API KEYS AND AVAILABLE MODELS
###############################################################################

api_keys:
  # ðŸ”¶ ANTHROPIC (CLAUDE) MODELS
  # API Key for all Claude models
  anthropic:
    api_key: "your-anthropic-api-key-here"  # Get from: https://console.anthropic.com
    max_tokens: 4096
    temperature: 0.7

    # Available Models:
    models:
      claude-4.1-opus:
        cost_per_1m: "Input: $15.00 / Output: $75.00"
        description: "Complex reasoning, autonomous agents, high-stakes analysis"
        interview_quality: "Excellent"
        use_cases: ["Complex interviews", "Deep analysis", "High-quality responses"]

      claude-4.5-sonnet:
        cost_per_1m: "Input: $3.00 / Output: $15.00"
        description: "Agentic workhorse, advanced coding, intelligent applications"
        interview_quality: "Excellent"
        use_cases: ["Standard interviews", "Balanced cost/quality", "Recommended"]
        recommended: true

      claude-4.5-haiku:
        cost_per_1m: "Input: $1.00 / Output: $5.00"
        description: "Near-frontier speed, fast customer apps, intelligent search"
        interview_quality: "Very Good"
        use_cases: ["Fast interviews", "High volume", "Cost optimization"]

      claude-3-haiku:
        cost_per_1m: "Input: $0.25 / Output: $1.25"
        description: "Ultra-fast, simple Q&A, content moderation, routing"
        interview_quality: "Good for Testing"
        use_cases: ["Testing", "Prototyping", "Budget-constrained"]

  # ðŸ”· OPENAI (GPT-5) MODELS
  # API Key for all OpenAI models
  openai:
    api_key: "your-openai-api-key-here"  # Get from: https://platform.openai.com/api-keys
    max_tokens: 4096
    temperature: 0.7

    # Available Models:
    models:
      gpt-5-pro:
        cost_per_1m: "Input: $15.00 / Output: $120.00"
        description: "Peak performance, mission-critical reasoning, high-precision tasks"
        interview_quality: "Excellent"
        use_cases: ["Premium interviews", "Critical accuracy", "Complex reasoning"]

      gpt-5:
        cost_per_1m: "Input: $1.25 / Output: $10.00"
        description: "Advanced tasks, complex coding, agentic workflows"
        interview_quality: "Excellent"
        use_cases: ["Standard interviews", "Advanced reasoning", "Quality work"]
        recommended: true

      gpt-5-mini:
        cost_per_1m: "Input: $0.25 / Output: $2.00"
        description: "Capable & fast, smart chatbots, data analysis, daily-driver tasks"
        interview_quality: "Very Good"
        use_cases: ["Fast interviews", "Good value", "High volume"]

      gpt-5-nano:
        cost_per_1m: "Input: $0.05 / Output: $0.40"
        description: "High-throughput, classification, summarization, simple tasks"
        interview_quality: "Good for Testing"
        use_cases: ["Testing", "Simple tasks", "Maximum cost savings"]

  # ðŸŸ¢ GOOGLE (GEMINI) MODELS
  # API Key for all Gemini models
  google:
    api_key: "your-google-api-key-here"  # Get from: https://makersuite.google.com/app/apikey
    max_tokens: 4096
    temperature: 0.7

    # Available Models:
    models:
      gemini-2.5-pro:
        cost_per_1m: "Input: $1.25 / Output: $10.00"
        description: "Advanced 'thinking' model, complex coding, difficult instruction-following"
        interview_quality: "Excellent"
        use_cases: ["Complex interviews", "Advanced reasoning", "Quality responses"]
        recommended: true

      gemini-2.5-flash:
        cost_per_1m: "Input: $0.30 / Output: $2.50"
        description: "Enterprise workhorse, high-volume agents, fast & capable chat"
        interview_quality: "Very Good"
        use_cases: ["Fast interviews", "High volume", "Good balance"]

      gemini-2.5-flash-lite:
        cost_per_1m: "Input: $0.10 / Output: $0.40"
        description: "High-volume, low-latency, classification, simple chatbots"
        interview_quality: "Good for Testing"
        use_cases: ["Testing", "Simple interviews", "Budget option"]

      gemini-2.0-flash:
        cost_per_1m: "Input: $0.05 / Output: $0.20"
        description: "Legacy model, experimental, basic task prototyping"
        interview_quality: "Good for Testing"
        use_cases: ["Prototyping", "Experimentation", "Lowest cost"]

# HuggingFace Configuration
huggingface:
  token: "your-huggingface-token-here"  # Optional, for private datasets
  dataset: "argilla/FinePersonas-v0.1"
  cache_dir: "./data/hf_cache"

# Data Paths
data_paths:
  personas: "./data/personas"
  health_records: "./data/health_records"
  matched: "./data/matched"
  interviews: "./data/interviews"
  synthea_output: "./synthea/output/fhir"

# Persona Retrieval Settings
persona_settings:
  target_count: 10000
  age_range:
    min: 12
    max: 60
  gender_filter: "female"
  fertile_age_keywords:
    - "woman"
    - "female"
    - "mother"
    - "pregnant"
    - "childbearing"

# Synthea Configuration
synthea:
  executable: "./synthea/run_synthea"  # or run_synthea.bat on Windows
  population_size: 10000
  state: "Massachusetts"  # Can be changed to any US state
  pregnancy_modules:
    - "pregnancy"
    - "prenatal"
    - "maternal_health"
  snomed_codes:
    # Pregnancy-related SNOMED codes
    - "77386006"   # Pregnancy
    - "72892002"   # Normal pregnancy
    - "364320009"  # Pregnancy observable
    - "249166004"  # Antenatal care
    - "237238006"  # Maternal care
    - "169560008"  # Prenatal visit
    - "118185001"  # Finding related to pregnancy
  output_format: "fhir"

# Matching Algorithm Settings
matching:
  algorithm: "weighted"  # Options: weighted, random, optimized
  weights:
    age_compatibility: 0.6
    socioeconomic_factors: 0.4
  age_tolerance: 2  # years difference allowed
  socioeconomic_factors:
    - "education"
    - "occupation"
    - "income_level"
    - "marital_status"

# Interview Settings
interview:
  # Note: active_provider and active_model at top of file override this
  default_provider: "anthropic"  # Options: anthropic, openai, google
  protocol_path: "./Script/interview_protocols"
  max_turns: 20
  save_transcripts: true
  batch_size: 10  # Number of interviews to run in parallel

# Logging
logging:
  level: "INFO"  # DEBUG, INFO, WARNING, ERROR
  file: "./logs/pipeline.log"
  console: true

# Performance
performance:
  max_workers: 4  # Parallel processing threads
  batch_size: 100
  cache_enabled: true

###############################################################################
# OPTIONAL SETTINGS (from env.example)
###############################################################################
optional:
  hf_hub_enable_hf_transfer: true
  persona_filter_confidence: 0.8
  data_dir: "data"
  log_dir: "logs"
  output_dir: "outputs"
