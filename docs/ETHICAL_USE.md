# Ethical Use Guidelines - v1.2.1

**Project:** Gravidas - Persona-to-Health-Record Matching System
**Phase:** Phase 3, Task 3.2 - Ethical Use Guidelines
**Version:** 1.2.1
**Date:** 2025-11-16
**Status:** ✅ COMPLETE

---

## Table of Contents

1. [Purpose of This Document](#purpose-of-this-document)
2. [Core Ethical Principles](#core-ethical-principles)
3. [Appropriate Use Cases](#appropriate-use-cases)
4. [Inappropriate Use Cases](#inappropriate-use-cases)
5. [Limitations and Disclaimers](#limitations-and-disclaimers)
6. [Bias Acknowledgment](#bias-acknowledgment)
7. [Citation Requirements](#citation-requirements)
8. [Data Privacy and Security](#data-privacy-and-security)
9. [Responsible AI Practices](#responsible-ai-practices)
10. [Reporting Misuse](#reporting-misuse)

---

## Purpose of This Document

This document provides **ethical guidelines for the responsible use of Gravidas**, a synthetic healthcare interview generation system. Gravidas generates **synthetic (artificial) maternal health data** for research, training, and algorithm development.

### ⚠️ Critical Understanding

**Gravidas data is NOT real patient data.** All personas, health records, and interviews are synthetically generated using AI and simulation tools. This data:

✅ **CAN** be used for research, training, and algorithm development
❌ **CANNOT** be presented as real patient data
❌ **CANNOT** be used for clinical decision-making

---

## Core Ethical Principles

### 1. Transparency

Users must **clearly disclose** that data is synthetic when publishing, presenting, or sharing results.

**Example Disclosure:**
> "This study uses synthetic maternal health data generated by the Gravidas system (v1.2.1), which creates artificial personas and simulated clinical interviews. No real patient data was used in this analysis."

### 2. Non-Maleficence (Do No Harm)

Synthetic data must **not be used** in ways that could:
- Mislead healthcare providers
- Influence real clinical decisions
- Misrepresent actual patient populations
- Perpetuate harmful stereotypes

### 3. Academic Integrity

Research using Gravidas must:
- Accurately represent data provenance (synthetic)
- Acknowledge limitations
- Not overclaim validity or generalizability
- Cite source appropriately

### 4. Privacy Respect

Even though data is synthetic, the system is designed to avoid:
- Identifiable combinations that resemble real individuals
- Sensitive cultural or ethnic stereotypes
- Privacy-invasive questioning patterns

### 5. Scientific Rigor

Research must:
- Document methodology completely
- Report synthetic data characteristics
- Acknowledge that findings may not reflect real populations
- Validate against real data when possible

---

## Appropriate Use Cases

### ✅ Acceptable Uses

Gravidas is designed and approved for the following purposes:

#### 1. **Algorithm Development & Testing**

**Use:** Developing and testing healthcare AI systems before deployment on real data.

**Examples:**
- Training natural language processing models
- Testing clinical decision support algorithms
- Developing risk prediction models
- Prototyping patient interaction systems

**Why Appropriate:**
- Large-scale synthetic data enables rapid iteration
- No privacy concerns during development
- Identifies technical issues before real deployment

**Example:**
```python
# Acceptable: Training an NLP model on synthetic interviews
from transformers import AutoTokenizer, AutoModelForSequenceClassification

# Load synthetic interview data
interviews = load_interviews('data/interviews')

# Train model
model = train_sentiment_model(interviews)

# Document in paper:
# "Model was initially trained on 1,000 synthetic interviews
#  generated by Gravidas v1.2.1, then fine-tuned on 100 real
#  interviews from [IRB-approved study]."
```

#### 2. **Educational & Training Purposes**

**Use:** Teaching healthcare providers, researchers, or students.

**Examples:**
- Medical education simulations
- Interview technique training
- Healthcare informatics courses
- Research methodology demonstrations

**Why Appropriate:**
- No patient privacy violations
- Controlled scenarios for learning
- Scalable training material

**Example Course Use:**
> "In this module, you'll practice clinical interviewing skills using synthetic patient personas generated by Gravidas. These realistic scenarios allow you to develop skills in a safe environment before working with real patients."

#### 3. **Operations Research & Cost Analysis**

**Use:** Studying healthcare system operations, costs, and resource allocation.

**Examples:**
- LLM cost optimization studies
- Multi-provider pricing comparisons
- Resource allocation modeling
- Workflow efficiency analysis

**Why Appropriate:**
- Focus on operational metrics, not clinical validity
- Demonstrates cost-effectiveness of AI systems
- Informs procurement decisions

**Example Research Question:**
> "What is the cost-per-interview for different LLM providers when conducting prenatal care interviews at scale?"

#### 4. **User Experience (UX) Research**

**Use:** Testing interview flows, question design, and system usability.

**Examples:**
- Testing interview protocol effectiveness
- Evaluating question comprehension
- Assessing conversation flow
- Identifying technical issues

**Why Appropriate:**
- Focuses on system design, not clinical outcomes
- Identifies usability problems early
- No risk to real patients

#### 5. **System Prototyping & Proof-of-Concept**

**Use:** Demonstrating feasibility of healthcare AI systems.

**Examples:**
- Grant proposals showing concept viability
- Technical demonstrations for stakeholders
- Pilot studies before IRB approval
- Technology assessment

**Why Appropriate:**
- Rapid prototyping without IRB delays
- Demonstrates technical capability
- Informs real study design

#### 6. **Research Methodology Development**

**Use:** Developing and refining research methodologies.

**Examples:**
- Testing data collection instruments
- Validating analytical methods
- Piloting study protocols
- Developing coding schemes

**Why Appropriate:**
- Methodological focus
- Identifies design flaws
- Prepares for real data collection

#### 7. **Public Datasets & Benchmarks**

**Use:** Creating standardized datasets for research comparisons.

**Examples:**
- Benchmark datasets for algorithm comparison
- Open datasets for reproducibility
- Competition datasets (Kaggle, etc.)

**Why Appropriate:**
- Enables research without privacy barriers
- Promotes reproducibility
- Facilitates method comparison

**Important:** Clearly label as synthetic in all publications.

---

## Inappropriate Use Cases

### ❌ Prohibited Uses

The following uses of Gravidas are **explicitly prohibited** and violate ethical guidelines:

#### 1. **Clinical Decision-Making**

**❌ Prohibited:** Using Gravidas data to make real patient care decisions.

**Examples of Misuse:**
- Training a clinical AI system ONLY on synthetic data, then deploying on real patients without validation
- Using synthetic data to inform treatment protocols
- Basing resource allocation decisions on synthetic population characteristics

**Why Prohibited:**
- Synthetic data does not reflect real patient complexity
- May lead to patient harm
- Violates clinical standards of care

**What to Do Instead:**
> Always validate systems trained on synthetic data using real clinical data before deployment. Synthetic data is for development ONLY, not deployment.

#### 2. **Presenting as Real Data**

**❌ Prohibited:** Claiming or implying that synthetic data represents real patients.

**Examples of Misuse:**
- Publishing "patient interviews" without disclosing they are synthetic
- Presenting demographic statistics as if from real population
- Using synthetic data in case studies without clear synthetic label

**Why Prohibited:**
- Academic fraud
- Misleads readers and policymakers
- Damages scientific credibility

**Consequences:**
- Journal retraction
- Loss of funding
- Professional sanctions

#### 3. **Policy Recommendations Without Real Data Validation**

**❌ Prohibited:** Making healthcare policy recommendations based solely on synthetic data.

**Examples of Misuse:**
- Recommending policy changes based on synthetic population needs
- Advocating for resource allocation based on synthetic cost analysis alone
- Proposing clinical guidelines from synthetic interviews

**Why Prohibited:**
- Policies must reflect real population needs
- Synthetic data may not capture real-world constraints
- Risk of harm to real populations

**What to Do Instead:**
> Use synthetic data for initial exploration, then validate findings with real data before making policy recommendations.

#### 4. **Insurance or Financial Decisions**

**❌ Prohibited:** Using synthetic data for insurance underwriting, pricing, or claims processing.

**Examples of Misuse:**
- Training insurance risk models on synthetic data
- Setting premiums based on synthetic population analysis
- Denying claims based on synthetic care patterns

**Why Prohibited:**
- Legal and regulatory violations
- Discriminatory outcomes
- Financial harm to real individuals

#### 5. **Diagnosing or Treating Real Patients**

**❌ Prohibited:** Using any component of Gravidas to diagnose, treat, or advise real patients.

**Examples of Misuse:**
- Deploying interview chatbot trained on synthetic data to interact with real patients
- Using synthetic care patterns to guide real patient care
- Presenting synthetic health information to patients

**Why Prohibited:**
- Not validated for clinical use
- May cause patient harm
- Liability and malpractice concerns

#### 6. **Surveillance or Profiling**

**❌ Prohibited:** Using Gravidas to profile, surveil, or identify real individuals.

**Examples of Misuse:**
- Attempting to match synthetic personas to real individuals
- Using patterns from synthetic data to profile populations
- Surveillance of pregnancy or reproductive choices

**Why Prohibited:**
- Privacy violations
- Ethical violations
- Potential for discrimination

#### 7. **Misrepresenting Research Findings**

**❌ Prohibited:** Overstating the validity or generalizability of findings from synthetic data.

**Examples of Misuse:**
- Claiming "interview study of 1,000 pregnant women" without specifying synthetic
- Stating "patients reported..." when referring to AI-generated responses
- Publishing in clinical journals without clear synthetic data disclosure

**Why Prohibited:**
- Academic misconduct
- Misleads clinical community
- Damages trust in research

---

## Limitations and Disclaimers

### ⚠️ Important Limitations

Users must acknowledge and understand the following limitations:

#### 1. **Synthetic Data Limitations**

**Limitation:** Synthetic data does not capture real-world complexity.

**Specific Issues:**
- AI-generated personas may not reflect actual patient diversity
- Simulated interviews lack authentic patient experiences
- Health records from Synthea may oversimplify conditions
- Matching algorithm may not reflect real patient-provider assignments

**Implication:**
Findings from Gravidas data **cannot be directly generalized to real populations** without validation.

**Required Disclosure:**
> "This study uses synthetic data that may not fully represent real patient populations. Findings should be validated with real clinical data before implementation."

#### 2. **AI Bias**

**Limitation:** LLM-generated personas may perpetuate AI biases.

**Specific Risks:**
- Stereotypical representations of demographic groups
- Overrepresentation of common patterns
- Underrepresentation of rare conditions or circumstances
- Western-centric assumptions in AI training data

**Implication:**
Synthetic data may **amplify existing biases** in AI training data.

**Mitigation:**
- Document demographic distribution
- Compare to real population statistics when available
- Explicitly acknowledge potential biases in publications

#### 3. **Clinical Validity**

**Limitation:** Synthetic clinical data has not been clinically validated.

**Specific Issues:**
- Interview responses are AI-generated, not from real patients
- Vital signs and lab values are simulated
- Disease progression patterns may be unrealistic
- Treatment patterns may not reflect real clinical practice

**Implication:**
Gravidas data **should not be used to draw clinical conclusions** without real data validation.

#### 4. **Temporal Validity**

**Limitation:** Clinical guidelines and LLM capabilities change over time.

**Specific Issues:**
- Interview protocols based on ACOG/ADA 2025 guidelines
- LLM models and pricing from November 2025
- Healthcare costs and practices evolve

**Implication:**
Results may become **outdated** as guidelines and technology evolve.

**Recommendation:**
> Document exact version (v1.2.1), date, and clinical guidelines used. Revalidate findings periodically.

#### 5. **Statistical Power**

**Limitation:** Synthetic data can generate arbitrary sample sizes.

**Specific Risk:**
- Easy to generate "statistically significant" findings with large N
- May not reflect real-world sample size constraints
- P-hacking through repeated synthetic generation

**Implication:**
Statistical significance in synthetic data **does not imply real-world significance**.

**Best Practice:**
> Report both synthetic and real data results. Do not rely solely on synthetic data for statistical inference.

#### 6. **Cultural and Linguistic Limitations**

**Limitation:** System is primarily designed for English-speaking populations.

**Specific Issues:**
- Interview protocols in English only
- LLM training data predominantly English
- Cultural assumptions may not transfer internationally
- Healthcare system assumptions (U.S.-centric)

**Implication:**
Results may not generalize to **non-English-speaking or non-U.S. populations**.

---

## Bias Acknowledgment

### Potential Sources of Bias

Gravidas users must acknowledge and mitigate the following sources of bias:

#### 1. **LLM Training Data Bias**

**Source:** Large language models (Claude, GPT, Gemini) are trained on internet text data that contains societal biases.

**Manifestations:**
- Stereotypical persona generation (e.g., linking low education with specific demographics)
- Biased health assumptions
- Reproductive choice assumptions
- Socioeconomic stereotypes

**Mitigation:**
```python
# Review generated personas for stereotypes
def check_for_stereotypes(personas):
    # Check for problematic correlations
    demographics = [
        (p['age'], p['education'], p['income'], p['race'])
        for p in personas
    ]

    # Flag if low education strongly correlates with specific race
    # Document and adjust generation prompts if needed
```

**Required Disclosure:**
> "Synthetic personas were generated using [Model], which may reflect biases present in its training data. We reviewed personas for stereotypical patterns and [describe any adjustments made]."

#### 2. **Synthea Simulation Bias**

**Source:** Synthea generates health records based on U.S. population statistics and disease models.

**Manifestations:**
- U.S.-specific healthcare patterns
- May not reflect international health systems
- Disease models may not capture all real-world variation

**Mitigation:**
- Document Synthea version and configuration
- Acknowledge U.S.-centric assumptions
- Compare to real population statistics when possible

#### 3. **Protocol Design Bias**

**Source:** Interview protocols based on ACOG/ADA guidelines reflect current medical consensus.

**Manifestations:**
- May not capture patient-centered concerns
- Reflects provider perspective more than patient perspective
- Standardization may miss individual variation

**Mitigation:**
- Acknowledge protocol limitations
- Consider piloting with patient advisory groups
- Document protocol design decisions

#### 4. **Matching Algorithm Bias**

**Source:** Semantic matching uses weighted factors that may not reflect all relevant dimensions.

**Manifestations:**
- Age weighted most heavily (40%)
- Occupation, race, other factors weighted less
- May not capture important cultural or social factors

**Mitigation:**
- Document matching weights
- Explore sensitivity to different weighting schemes
- Acknowledge that matching is artificial construct

#### 5. **Selection Bias**

**Source:** Choices in system design create systematic patterns.

**Manifestations:**
- Only females age 12-60 included (exclusion of older women, transgender individuals)
- Focus on pregnancy-related care (exclusion of other health needs)
- Assumption of healthcare access (system-engaged individuals)

**Mitigation:**
- Clearly define population scope
- Acknowledge excluded populations
- Do not overgeneralize findings

### Bias Reporting Template

When publishing research using Gravidas, include a bias statement:

```markdown
## Bias Statement

This study uses synthetic data generated by Gravidas v1.2.1, which has the following potential biases:

1. **LLM Bias:** Personas generated by [Model] may reflect biases in training data, including [specific concerns if identified].

2. **Population Bias:** Data represents [describe scope], excluding [describe exclusions].

3. **Geographic Bias:** Health records generated using Synthea reflect U.S. healthcare patterns and may not generalize to [other regions].

4. **Temporal Bias:** Data generated [date] using clinical guidelines current at that time.

We mitigated these biases by [describe mitigation steps] and acknowledge that findings may not generalize to [describe limitations].
```

---

## Citation Requirements

### How to Cite Gravidas

#### Academic Publications

**Full Citation:**
```
Polônia, D. (2025). Gravidas: A Synthetic Healthcare Interview Generation System for Maternal Health Research (Version 1.2.1) [Software]. GitHub. https://github.com/yourusername/202511-Gravidas
```

**In-Text Citation:**
```
Gravidas (Polônia, 2025)
```

**Bibtex:**
```bibtex
@software{polonia2025gravidas,
  author = {Polônia, Diogo},
  title = {Gravidas: A Synthetic Healthcare Interview Generation System for Maternal Health Research},
  year = {2025},
  version = {1.2.1},
  url = {https://github.com/yourusername/202511-Gravidas},
  note = {Software for generating synthetic maternal health interviews}
}
```

#### Data Citation

If publishing dataset generated by Gravidas:

```
Polônia, D. (2025). Synthetic Maternal Health Interview Dataset (Generated by Gravidas v1.2.1) [Dataset]. [Repository]. DOI: [if applicable]
```

### Required Acknowledgments

All publications using Gravidas must include:

1. **Version Number:** Specify exact version (e.g., v1.2.1)
2. **Synthetic Nature:** Clearly state data is synthetic
3. **Method Description:** Brief description of generation process
4. **Limitations:** Acknowledge synthetic data limitations

**Example Acknowledgment:**
> "This research utilized Gravidas v1.2.1 (Polônia, 2025), an open-source system for generating synthetic maternal health interviews. All data in this study is synthetically generated using AI-powered persona creation, Synthea health record simulation, and LLM-based interview generation. Findings should be interpreted as preliminary and validated with real clinical data before implementation."

### Citing Component Tools

Also cite the underlying tools:

**Synthea:**
```
Walonoski, J., et al. (2018). Synthea: An approach, method, and software mechanism for generating synthetic patients and the synthetic electronic health care record. Journal of the American Medical Informatics Association, 25(3), 230-238.
```

**Anthropic Claude (if used):**
```
Anthropic. (2025). Claude 4.5 [Large language model]. https://www.anthropic.com/
```

**OpenAI (if used):**
```
OpenAI. (2025). GPT-5 [Large language model]. https://openai.com/
```

---

## Data Privacy and Security

### Synthetic Data Privacy

**Key Point:** Even though Gravidas data is synthetic, treat it with appropriate care.

#### Best Practices

1. **Do Not Attempt to Re-identify**
   - Do not try to match synthetic personas to real individuals
   - Do not combine with real data in ways that could identify individuals

2. **Secure Storage**
   - Store synthetic data securely (even though it's not sensitive)
   - Use version control properly (.gitignore for data files)
   - Document data provenance

3. **Sharing Data**
   - Clearly label as synthetic when sharing
   - Include version and generation date
   - Provide full methodology for reproducibility

4. **API Keys**
   - Never commit API keys to version control
   - Use environment variables (.env file)
   - Rotate keys regularly

---

## Responsible AI Practices

### Ethical AI Development

When extending or modifying Gravidas:

#### 1. **Fairness**

- Test for demographic balance in generated personas
- Avoid reinforcing stereotypes in interview prompts
- Ensure diverse representation

```python
# Check demographic balance
def check_demographic_balance(personas):
    """Ensure diverse representation."""
    demographics = pd.DataFrame([
        p['semantic_tree']['demographics']
        for p in personas
    ])

    # Check for balance
    print(demographics.describe())
    print(demographics['ethnicity'].value_counts())

    # Flag if < 10% of any major group
    for group in demographics['ethnicity'].unique():
        prop = (demographics['ethnicity'] == group).mean()
        if prop < 0.10:
            logger.warning(f"Low representation: {group} ({prop:.1%})")
```

#### 2. **Transparency**

- Document all AI model choices
- Explain matching algorithm clearly
- Provide full code access (open source)

#### 3. **Accountability**

- Maintain version control
- Document all changes
- Enable reproducibility

#### 4. **Safety**

- Implement red flag detection in interviews
- Review for harmful content generation
- Test error handling

---

## Reporting Misuse

### If You Observe Misuse

If you become aware of Gravidas being used inappropriately:

1. **Document the Misuse**
   - Capture evidence (publication, presentation, etc.)
   - Note specific ethical violations

2. **Report Through Appropriate Channels**
   - Journal editors (if published research)
   - Institutional review boards
   - Project maintainers: [email protected]

3. **GitHub Issues**
   - Open an issue at: https://github.com/yourusername/202511-Gravidas/issues
   - Label as "ethical-concern"

### Handling Reports

Project maintainers will:
1. Investigate reported misuse
2. Contact users to request correction
3. Document in public issue tracker
4. Update guidelines if needed

---

## Conclusion

Gravidas is a powerful tool for healthcare AI research when used responsibly. By following these ethical guidelines, you contribute to:

✅ Advancing healthcare AI safely
✅ Maintaining academic integrity
✅ Protecting patient welfare
✅ Building trustworthy AI systems

### Key Takeaways

1. **Always disclose** that data is synthetic
2. **Never use** for clinical decisions without real data validation
3. **Acknowledge** limitations and biases
4. **Cite** properly in all publications
5. **Report** any misuse you observe

---

## Additional Resources

- **IRB Guidance:** Most synthetic data studies don't require IRB approval, but check with your institution
- **Research Ethics:** [Belmont Report](https://www.hhs.gov/ohrp/regulations-and-policy/belmont-report/index.html)
- **Responsible AI:** [IEEE Ethics Guidelines](https://ethicsinaction.ieee.org/)
- **Open Science:** [FAIR Principles](https://www.go-fair.org/fair-principles/)

---

**Document Prepared By:** Claude Code
**Date:** 2025-11-16
**Version:** 1.2.1
**Status:** Task 3.2 COMPLETE ✅

**Questions?** Open an issue on GitHub or contact the maintainers.

---

**By using Gravidas, you agree to follow these ethical guidelines and use the system responsibly.**
