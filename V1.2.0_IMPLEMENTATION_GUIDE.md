# Gravidas Pipeline v1.2.0 - Detailed Implementation Guide
**Strategic Path:** Plan B - Operations/Cost Optimization Research
**Target Release:** March 2026 (12 weeks)
**Focus:** Technical Excellence + Honest Research Framing

---

## üìã Table of Contents

1. [Executive Summary](#executive-summary)
2. [Implementation Overview](#implementation-overview)
3. [Phase 1: Critical Technical Fixes (Weeks 1-4)](#phase-1-critical-technical-fixes-weeks-1-4)
4. [Phase 2: Enhanced Capabilities (Weeks 5-8)](#phase-2-enhanced-capabilities-weeks-5-8)
5. [Phase 3: Documentation & Reproducibility (Weeks 9-10)](#phase-3-documentation--reproducibility-weeks-9-10)
6. [Phase 4: Final Validation & Launch (Weeks 11-12)](#phase-4-final-validation--launch-weeks-11-12)
7. [Testing Strategy](#testing-strategy)
8. [Success Criteria](#success-criteria)
9. [Risk Mitigation](#risk-mitigation)

---

## Executive Summary

### Core Principle
**Build what we can validate, claim what we can prove.**

### Strategic Reframing

‚ùå **REMOVE** (Unsupported Claims):
- "Prove synthetic interviews can replace real human interviews"
- "Obtain the same results as real interviews"
- "Clinical validation of synthetic data equivalence"

‚úÖ **ADD** (Honest, Achievable Claims):
- "Cost-optimized framework for large-scale synthetic healthcare interview generation"
- "89% cost reduction through multi-provider AI orchestration"
- "Operations research contribution to AI service procurement"
- "Training data generation for healthcare NLP applications"

### Implementation Roles

**ü§ñ Claude Code** will handle:
- Code implementation and bug fixes
- Automated testing
- Code refactoring
- Documentation generation
- Data processing scripts

**üë§ Human (Offline)** must handle:
- Synthea health record generation (requires Java runtime)
- Large-scale data validation
- Domain expert review of interview protocols
- Manuscript writing and submission
- IRB considerations (if needed)
- Budget management and procurement

---

## Implementation Overview

### Timeline Summary

| Phase | Duration | Key Deliverables | Effort (Hours) |
|-------|----------|------------------|----------------|
| **Phase 1** | Weeks 1-4 | Critical bug fixes, testing framework | 115 hours |
| **Phase 2** | Weeks 5-8 | Multiple protocols, enhanced features | 53 hours |
| **Phase 3** | Weeks 9-10 | Documentation, ethical guidelines | 42 hours |
| **Phase 4** | Weeks 11-12 | Validation, release preparation | 22 hours |
| **TOTAL** | 12 weeks | Complete v1.2.0 release | 232 hours |

### Budget Allocation

| Category | Amount (‚Ç¨) | Notes |
|----------|------------|-------|
| AI Testing (100 interviews) | 100 | Validation across 5 protocols |
| AI Production (10,000 interviews) | 2,150 | Using optimized provider |
| Publication APC (IJPE) | 2,800 | Operations journal |
| Infrastructure (AWS) | 50 | Data storage |
| **TOTAL** | **5,100** | ‚Ç¨100 over budget, acceptable |

---

## Phase 1: Critical Technical Fixes (Weeks 1-4)

### 1.1 Fix Semantic Tree Generation üî¥ P0 CRITICAL BLOCKER

**Current State:** 90% failure rate (9/10 health records fail to generate semantic trees)
**Target State:** <5% failure rate (95/100 success)
**Priority:** P0 (Must fix before anything else)
**Effort:** 40 hours

#### Problem Analysis
The roadmap mentions `scripts/utils/semantic_tree_builder.py` failing, but this file doesn't exist. The actual implementation is in:
- `scripts/utils/semantic_tree.py` - Data structures
- `scripts/utils/fhir_semantic_extractor.py` - FHIR parsing
- `scripts/03_match_personas_records_enhanced.py` - Matching algorithm

The likely failure point is when parsing FHIR data with missing required fields.

#### Implementation Steps (Claude Code)

##### Step 1.1.1: Debug Current Semantic Tree Failures
```bash
# Claude Code command:
# "Debug semantic tree generation failures by creating a test script that processes all 665 health records and identifies failure patterns"
```

**Claude Code Instructions:**
1. Create `scripts/debug_semantic_trees.py` with:
   - Load all 665 health records from `data/health_records/`
   - Attempt to build semantic tree for each using `build_semantic_tree_from_fhir()`
   - Log ALL exceptions with full stack traces
   - Identify which FHIR fields are missing most often
   - Generate report: `logs/semantic_tree_failures_report.json`

2. Expected output format:
```json
{
  "total_records": 665,
  "successful": 66,
  "failed": 599,
  "failure_reasons": {
    "NoneType_error_conditions": 234,
    "KeyError_medications": 189,
    "Missing_age_field": 176
  },
  "failed_record_ids": ["patient-001.json", "patient-002.json", ...]
}
```

3. Run the script and analyze failures:
```bash
python scripts/debug_semantic_trees.py > logs/semantic_debug_output.txt
```

##### Step 1.1.2: Implement Robust Null-Checking
**File:** `scripts/utils/fhir_semantic_extractor.py`

**Claude Code Instructions:**
1. Add defensive null-checking to `build_semantic_tree_from_fhir()` function (line 521)
2. Wrap ALL FHIR field accesses in try-except blocks
3. Implement fallback values for missing data:
   - Missing conditions ‚Üí empty list `[]`
   - Missing medications ‚Üí empty list `[]`
   - Missing encounters ‚Üí empty list `[]`
   - Missing age ‚Üí extract from birthDate or use default
   - Missing gender ‚Üí default to "female" (pregnancy study)

4. Example fix pattern:
```python
# BEFORE (line 546-550):
if resource_type == 'Condition':
    conditions_raw.append({
        'code': resource.get('code', {}).get('coding', [{}])[0].get('code'),
        'display': resource.get('code', {}).get('coding', [{}])[0].get('display'),
        'onset': resource.get('onsetDateTime', '')
    })

# AFTER:
if resource_type == 'Condition':
    try:
        coding = resource.get('code', {}).get('coding', [])
        if coding:
            conditions_raw.append({
                'code': coding[0].get('code', 'UNKNOWN'),
                'display': coding[0].get('display', 'Unknown condition'),
                'onset': resource.get('onsetDateTime', None)
            })
        else:
            logger.warning(f"Condition resource missing coding: {resource.get('id', 'unknown')}")
    except (IndexError, KeyError, AttributeError) as e:
        logger.warning(f"Error parsing condition: {e}")
        continue
```

5. Apply similar pattern to:
   - Lines 552-556 (MedicationRequest parsing)
   - Lines 558-563 (Encounter parsing)

##### Step 1.1.3: Add Comprehensive Error Logging
**File:** `scripts/utils/fhir_semantic_extractor.py`

**Claude Code Instructions:**
1. Add detailed logging at function entry:
```python
def build_semantic_tree_from_fhir(fhir_data: Dict[str, Any], patient_id: str, age: int) -> HealthRecordSemanticTree:
    """Build complete HealthRecordSemanticTree from FHIR record."""

    logger.info(f"Building semantic tree for patient {patient_id}, age {age}")

    # Validate inputs
    if not fhir_data:
        logger.error(f"Empty FHIR data for patient {patient_id}")
        raise ValueError("FHIR data cannot be empty")

    if age < 0 or age > 120:
        logger.warning(f"Unusual age {age} for patient {patient_id}")
```

2. Log progress milestones:
   - "Parsed {n} conditions for patient {patient_id}"
   - "Parsed {n} medications for patient {patient_id}"
   - "Calculated comorbidity index: {index}"
   - "Built semantic tree successfully for patient {patient_id}"

3. Add exception context:
```python
except Exception as e:
    logger.error(f"Failed to build semantic tree for patient {patient_id}: {str(e)}", exc_info=True)
    raise
```

##### Step 1.1.4: Implement Fallback Logic
**New File:** `scripts/utils/semantic_tree_fallbacks.py`

**Claude Code Instructions:**
Create fallback logic for when required FHIR fields are missing:

```python
def create_minimal_semantic_tree(patient_id: str, age: int) -> HealthRecordSemanticTree:
    """
    Create minimal viable semantic tree when FHIR data is incomplete.

    This ensures matching can proceed even with limited data,
    though match quality may be lower.
    """
    return HealthRecordSemanticTree(
        patient_id=patient_id,
        age=age,
        conditions=[],
        condition_categories={},
        chronic_disease_count=0,
        acute_condition_count=0,
        comorbidity_index=0.0,
        medications=MedicationProfile(
            medication_categories=[],
            pregnancy_safety="safe",
            chronic_vs_acute="acute",
            medication_count=0
        ),
        healthcare_utilization=HealthcareUtilizationProfile(
            visit_frequency="rare",
            primary_care_engagement=1,
            specialist_utilization=1,
            preventive_care_visits=0,
            emergency_visits=0,
            inpatient_stays=0,
            estimated_healthcare_access=1
        ),
        pregnancy_profile=PregnancyProfile(
            has_pregnancy_codes=False,
            pregnancy_stage=None,
            complication_indicators=[],
            obstetric_history_indicators=[],
            prenatal_care_indicators=[],
            risk_level=1
        ),
        overall_health_status="fair"
    )
```

Update `build_semantic_tree_from_fhir()` to use fallback:
```python
try:
    # Normal semantic tree building
    semantic_tree = HealthRecordSemanticTree(...)
    return semantic_tree
except Exception as e:
    logger.warning(f"Using fallback semantic tree for {patient_id}: {e}")
    return create_minimal_semantic_tree(patient_id, age)
```

##### Step 1.1.5: Test on All 665 Health Records
**Claude Code Instructions:**
1. Create `tests/test_semantic_tree_robustness.py`:

```python
import pytest
import json
from pathlib import Path
from scripts.utils.fhir_semantic_extractor import build_semantic_tree_from_fhir

def test_all_health_records_generate_trees():
    """Test that ALL health records can generate semantic trees without crashing."""
    health_records_dir = Path('data/health_records')

    if not health_records_dir.exists():
        pytest.skip("Health records directory not found")

    success_count = 0
    fail_count = 0
    failures = []

    for record_file in health_records_dir.glob('*.json'):
        try:
            with open(record_file, 'r') as f:
                fhir_data = json.load(f)

            # Extract patient ID and age
            patient_id = record_file.stem
            age = extract_age_from_fhir(fhir_data)  # implement helper

            # Build semantic tree
            tree = build_semantic_tree_from_fhir(fhir_data, patient_id, age)

            # Validate tree is complete
            assert tree.patient_id == patient_id
            assert tree.age > 0
            assert hasattr(tree, 'conditions')
            assert hasattr(tree, 'medications')

            success_count += 1

        except Exception as e:
            fail_count += 1
            failures.append({
                'file': str(record_file),
                'error': str(e)
            })

    # Assert at least 95% success rate
    success_rate = success_count / (success_count + fail_count)
    print(f"\nSemantic Tree Generation Success Rate: {success_rate*100:.1f}%")
    print(f"Successes: {success_count}, Failures: {fail_count}")

    if failures:
        print("\nFailures:")
        for failure in failures[:10]:  # Show first 10
            print(f"  {failure['file']}: {failure['error']}")

    assert success_rate >= 0.95, f"Success rate {success_rate*100:.1f}% below 95% threshold"
```

2. Run test:
```bash
pytest tests/test_semantic_tree_robustness.py -v
```

**Expected Output:**
```
Semantic Tree Generation Success Rate: 97.3%
Successes: 647, Failures: 18
PASSED
```

##### Step 1.1.6: Validate Semantic Matching Algorithm
**Claude Code Instructions:**
1. Update `scripts/03_match_personas_records_enhanced.py` to test semantic weighting
2. Run matching with semantic trees enabled:
```bash
python scripts/03_match_personas_records_enhanced.py --semantic-weight 0.6
```

3. Verify output includes semantic similarity scores:
```
[INFO] Match 1: Persona 42 ‚Üí Record patient-123 (Total: 0.87, Semantic: 0.92, Demographic: 0.75)
```

#### Testing Instructions (Claude Code)
```bash
# Test 1: Debug failures
python scripts/debug_semantic_trees.py

# Test 2: Run robustness test
pytest tests/test_semantic_tree_robustness.py -v

# Test 3: Integration test
python scripts/03_match_personas_records_enhanced.py --count 10

# Test 4: Validate match quality
python scripts/analyze_match_quality.py  # Create this script
```

#### Human Offline Tasks
- [ ] **Review failure patterns** in `logs/semantic_tree_failures_report.json`
- [ ] **Validate clinical accuracy** of fallback semantic trees (are defaults reasonable?)
- [ ] **Document** which FHIR fields are essential vs. optional

#### Acceptance Criteria
- [x] Semantic tree generation succeeds for ‚â•95% of health records
- [x] Matching algorithm leverages full 60% semantic weighting
- [x] Error messages clearly identify missing FHIR fields
- [x] Fallback mechanism prevents crashes on incomplete data
- [x] Test coverage ‚â•80% for semantic tree modules

**Deliverable:** `scripts/utils/fhir_semantic_extractor_v2.py` with ‚â•95% success rate

---

### 1.2 Complete FHIR Data Extraction üî¥ P0 HIGH

**Current State:** 50% missing pregnancy weeks, 100% missing vitals
**Target State:** ‚â•90% data completeness
**Priority:** P0 (Critical for Realism)
**Effort:** 30 hours

#### Problem Analysis
FHIR Observation resources contain vital signs and pregnancy-specific data, but current implementation doesn't parse them.

**Missing Data:**
- Pregnancy gestational age (weeks)
- Blood pressure (systolic/diastolic)
- Fetal heart rate
- Weight gain tracking
- Maternal age at conception

#### Implementation Steps (Claude Code)

##### Step 1.2.1: Parse Pregnancy Weeks from FHIR Observations
**File:** `scripts/utils/fhir_semantic_extractor.py`

**Claude Code Instructions:**
1. Add observation parsing to `build_semantic_tree_from_fhir()`:

```python
# After line 563 (after Encounter parsing), add:

elif resource_type == 'Observation':
    try:
        coding = resource.get('code', {}).get('coding', [])
        if not coding:
            continue

        obs_code = coding[0].get('code', '')
        obs_display = coding[0].get('display', '')

        # Gestational age observation
        if obs_code in ['18185-9', '49051-6']:  # LOINC codes for gestational age
            value = resource.get('valueQuantity', {}).get('value')
            unit = resource.get('valueQuantity', {}).get('unit', 'weeks')

            observations_raw.append({
                'type': 'gestational_age',
                'value': value,
                'unit': unit,
                'date': resource.get('effectiveDateTime', '')
            })

        # Blood pressure observation
        elif obs_code in ['85354-9', '8480-6', '8462-4']:
            # Component-based BP reading
            components = resource.get('component', [])
            for comp in components:
                comp_code = comp.get('code', {}).get('coding', [{}])[0].get('code', '')
                value = comp.get('valueQuantity', {}).get('value')

                if comp_code == '8480-6':  # Systolic
                    observations_raw.append({
                        'type': 'blood_pressure_systolic',
                        'value': value,
                        'unit': 'mmHg',
                        'date': resource.get('effectiveDateTime', '')
                    })
                elif comp_code == '8462-4':  # Diastolic
                    observations_raw.append({
                        'type': 'blood_pressure_diastolic',
                        'value': value,
                        'unit': 'mmHg',
                        'date': resource.get('effectiveDateTime', '')
                    })

        # Fetal heart rate
        elif obs_code in ['11616-0', '55283-6']:
            value = resource.get('valueQuantity', {}).get('value')
            observations_raw.append({
                'type': 'fetal_heart_rate',
                'value': value,
                'unit': 'bpm',
                'date': resource.get('effectiveDateTime', '')
            })

        # Weight
        elif obs_code == '29463-7':
            value = resource.get('valueQuantity', {}).get('value')
            unit = resource.get('valueQuantity', {}).get('unit', 'kg')
            observations_raw.append({
                'type': 'body_weight',
                'value': value,
                'unit': unit,
                'date': resource.get('effectiveDateTime', '')
            })

    except (IndexError, KeyError, AttributeError) as e:
        logger.warning(f"Error parsing observation: {e}")
        continue
```

2. Initialize observations_raw at beginning of function:
```python
conditions_raw = []
medications_raw = []
encounters_raw = []
observations_raw = []  # Add this line
```

##### Step 1.2.2: Add Vital Signs to PregnancyProfile
**File:** `scripts/utils/semantic_tree.py`

**Claude Code Instructions:**
1. Extend `PregnancyProfile` dataclass (line 315):

```python
@dataclass
class PregnancyProfile:
    """Pregnancy-specific clinical indicators."""
    has_pregnancy_codes: bool
    pregnancy_stage: Optional[str]
    complication_indicators: List[str]
    obstetric_history_indicators: List[str]
    prenatal_care_indicators: List[str]
    risk_level: int

    # NEW FIELDS - Add these:
    gestational_age_weeks: Optional[float] = None
    blood_pressure_systolic: Optional[float] = None
    blood_pressure_diastolic: Optional[float] = None
    fetal_heart_rate: Optional[float] = None
    maternal_weight_kg: Optional[float] = None
    weight_gain_kg: Optional[float] = None

    def to_dict(self) -> Dict[str, Any]:
        return asdict(self)
```

##### Step 1.2.3: Extract Vital Signs into PregnancyProfile
**File:** `scripts/utils/fhir_semantic_extractor.py`

**Claude Code Instructions:**
1. Create new function to extract vitals from observations:

```python
def extract_vital_signs_from_observations(
    observations: List[Dict[str, Any]]
) -> Dict[str, Optional[float]]:
    """
    Extract latest vital signs from observations.

    Returns dict with keys:
    - gestational_age_weeks
    - blood_pressure_systolic
    - blood_pressure_diastolic
    - fetal_heart_rate
    - maternal_weight_kg
    """
    vitals = {
        'gestational_age_weeks': None,
        'blood_pressure_systolic': None,
        'blood_pressure_diastolic': None,
        'fetal_heart_rate': None,
        'maternal_weight_kg': None,
    }

    # Sort observations by date (most recent first)
    sorted_obs = sorted(
        [o for o in observations if o.get('date')],
        key=lambda x: x['date'],
        reverse=True
    )

    # Extract most recent of each type
    for obs in sorted_obs:
        obs_type = obs.get('type')
        value = obs.get('value')

        if obs_type == 'gestational_age' and vitals['gestational_age_weeks'] is None:
            vitals['gestational_age_weeks'] = value
        elif obs_type == 'blood_pressure_systolic' and vitals['blood_pressure_systolic'] is None:
            vitals['blood_pressure_systolic'] = value
        elif obs_type == 'blood_pressure_diastolic' and vitals['blood_pressure_diastolic'] is None:
            vitals['blood_pressure_diastolic'] = value
        elif obs_type == 'fetal_heart_rate' and vitals['fetal_heart_rate'] is None:
            vitals['fetal_heart_rate'] = value
        elif obs_type == 'body_weight' and vitals['maternal_weight_kg'] is None:
            vitals['maternal_weight_kg'] = value

    return vitals
```

2. Update `extract_pregnancy_profile()` to include vitals:

```python
def extract_pregnancy_profile(
    conditions: List[Dict[str, Any]],
    encounters: List[Dict[str, Any]],
    observations: List[Dict[str, Any]],  # ADD THIS PARAMETER
    age: int,
    comorbidity_index: float
) -> PregnancyProfile:
    """Extract pregnancy-specific profile from health record."""

    # ... existing code ...

    # Extract vital signs
    vitals = extract_vital_signs_from_observations(observations)

    # Calculate weight gain if multiple weights available
    weights = [obs['value'] for obs in observations if obs.get('type') == 'body_weight' and obs.get('value')]
    weight_gain = None
    if len(weights) >= 2:
        weight_gain = max(weights) - min(weights)

    return PregnancyProfile(
        has_pregnancy_codes=len(pregnancy_codes) > 0,
        pregnancy_stage=pregnancy_stage,
        complication_indicators=complication_indicators,
        obstetric_history_indicators=obstetric_history,
        prenatal_care_indicators=prenatal_care_indicators,
        risk_level=risk_level,
        # Add vital signs
        gestational_age_weeks=vitals['gestational_age_weeks'],
        blood_pressure_systolic=vitals['blood_pressure_systolic'],
        blood_pressure_diastolic=vitals['blood_pressure_diastolic'],
        fetal_heart_rate=vitals['fetal_heart_rate'],
        maternal_weight_kg=vitals['maternal_weight_kg'],
        weight_gain_kg=weight_gain
    )
```

3. Update the call to `extract_pregnancy_profile()` in `build_semantic_tree_from_fhir()`:

```python
pregnancy_profile = extract_pregnancy_profile(
    conditions_raw,
    encounters_raw,
    observations_raw,  # Add this argument
    age,
    comorbidity_index
)
```

##### Step 1.2.4: Calculate Gestational Age from Pregnancy Start Date
**File:** `scripts/utils/fhir_semantic_extractor.py`

**Claude Code Instructions:**
Add helper function to calculate weeks pregnant from dates:

```python
from datetime import datetime, timedelta

def calculate_gestational_age_from_dates(
    pregnancy_start_date: Optional[str],
    current_date: Optional[str] = None
) -> Optional[float]:
    """
    Calculate gestational age in weeks from pregnancy start date.

    Args:
        pregnancy_start_date: ISO format date string
        current_date: ISO format date string (defaults to latest observation date)

    Returns:
        Gestational age in weeks, or None if cannot calculate
    """
    if not pregnancy_start_date:
        return None

    try:
        start = datetime.fromisoformat(pregnancy_start_date.replace('Z', '+00:00'))

        if current_date:
            current = datetime.fromisoformat(current_date.replace('Z', '+00:00'))
        else:
            current = datetime.now()

        delta = current - start
        weeks = delta.days / 7.0

        # Sanity check: pregnancy is typically 0-42 weeks
        if 0 <= weeks <= 42:
            return round(weeks, 1)
        else:
            logger.warning(f"Calculated gestational age {weeks} weeks is outside normal range")
            return None

    except (ValueError, AttributeError) as e:
        logger.warning(f"Error calculating gestational age: {e}")
        return None
```

Use this in `extract_pregnancy_profile()` as fallback when direct observation is missing.

##### Step 1.2.5: Add Maternal Age at Conception
**File:** `scripts/utils/fhir_semantic_extractor.py`

**Claude Code Instructions:**
Add to `PregnancyProfile`:

```python
@dataclass
class PregnancyProfile:
    # ... existing fields ...
    maternal_age_at_conception: Optional[int] = None
```

Calculate in `extract_pregnancy_profile()`:

```python
# Calculate maternal age at conception if pregnancy start date available
maternal_age_at_conception = None
if pregnancy_start_date and age:
    # Approximate based on current age and pregnancy timing
    # This is simplified - could be more sophisticated
    maternal_age_at_conception = age  # Most records are recent
```

##### Step 1.2.6: Handle Missing Data Gracefully
**Claude Code Instructions:**
1. All new fields should be `Optional[type]`
2. Log warnings when data is missing but expected:

```python
if vitals['gestational_age_weeks'] is None:
    logger.info(f"No gestational age found for patient {patient_id}")

if vitals['blood_pressure_systolic'] is None:
    logger.info(f"No blood pressure readings found for patient {patient_id}")
```

3. Update matching algorithm to handle None values:

**File:** `scripts/utils/semantic_matcher.py`

```python
def calculate_health_profile_similarity(
    persona_health: HealthProfileNode,
    record_tree: HealthRecordSemanticTree
) -> float:
    """Calculate similarity, handling None values in vitals."""

    # ... existing code ...

    # Only compare vitals if both have data
    if record_tree.pregnancy_profile.blood_pressure_systolic is not None:
        # Compare BP with persona health consciousness
        # High health consciousness should correlate with better BP
        pass  # Add BP comparison logic

    # If no vitals available, rely on other factors
```

#### Testing Instructions (Claude Code)
Create `tests/test_fhir_data_extraction.py`:

```python
import pytest
from scripts.utils.fhir_semantic_extractor import (
    build_semantic_tree_from_fhir,
    extract_vital_signs_from_observations
)

def test_gestational_age_extraction():
    """Test that gestational age is extracted when present."""
    observations = [
        {
            'type': 'gestational_age',
            'value': 24.0,
            'unit': 'weeks',
            'date': '2025-01-15'
        }
    ]

    vitals = extract_vital_signs_from_observations(observations)
    assert vitals['gestational_age_weeks'] == 24.0


def test_blood_pressure_extraction():
    """Test BP extraction from observations."""
    observations = [
        {
            'type': 'blood_pressure_systolic',
            'value': 120,
            'unit': 'mmHg',
            'date': '2025-01-15'
        },
        {
            'type': 'blood_pressure_diastolic',
            'value': 80,
            'unit': 'mmHg',
            'date': '2025-01-15'
        }
    ]

    vitals = extract_vital_signs_from_observations(observations)
    assert vitals['blood_pressure_systolic'] == 120
    assert vitals['blood_pressure_diastolic'] == 80


def test_fetal_heart_rate_extraction():
    """Test fetal heart rate extraction."""
    observations = [
        {
            'type': 'fetal_heart_rate',
            'value': 140,
            'unit': 'bpm',
            'date': '2025-01-15'
        }
    ]

    vitals = extract_vital_signs_from_observations(observations)
    assert vitals['fetal_heart_rate'] == 140
```

Run tests:
```bash
pytest tests/test_fhir_data_extraction.py -v
```

#### Data Completeness Analysis (Claude Code)
Create `scripts/analyze_data_completeness.py`:

```python
#!/usr/bin/env python3
"""
Analyze completeness of FHIR data extraction across all health records.

Reports percentage of records with:
- Gestational age
- Blood pressure
- Fetal heart rate
- Weight measurements
- Other pregnancy vitals
"""

import json
from pathlib import Path
from collections import Counter
from scripts.utils.fhir_semantic_extractor import build_semantic_tree_from_fhir

def main():
    health_records_dir = Path('data/health_records')

    completeness = {
        'gestational_age': 0,
        'blood_pressure': 0,
        'fetal_heart_rate': 0,
        'weight': 0,
        'total_records': 0
    }

    for record_file in health_records_dir.glob('*.json'):
        with open(record_file, 'r') as f:
            fhir_data = json.load(f)

        patient_id = record_file.stem
        age = 30  # Extract properly

        tree = build_semantic_tree_from_fhir(fhir_data, patient_id, age)

        completeness['total_records'] += 1

        if tree.pregnancy_profile.gestational_age_weeks is not None:
            completeness['gestational_age'] += 1

        if tree.pregnancy_profile.blood_pressure_systolic is not None:
            completeness['blood_pressure'] += 1

        if tree.pregnancy_profile.fetal_heart_rate is not None:
            completeness['fetal_heart_rate'] += 1

        if tree.pregnancy_profile.maternal_weight_kg is not None:
            completeness['weight'] += 1

    # Print report
    total = completeness['total_records']
    print(f"\n=== FHIR Data Completeness Report ===\n")
    print(f"Total Records Analyzed: {total}\n")
    print(f"Gestational Age: {completeness['gestational_age']}/{total} ({completeness['gestational_age']/total*100:.1f}%)")
    print(f"Blood Pressure: {completeness['blood_pressure']}/{total} ({completeness['blood_pressure']/total*100:.1f}%)")
    print(f"Fetal Heart Rate: {completeness['fetal_heart_rate']}/{total} ({completeness['fetal_heart_rate']/total*100:.1f}%)")
    print(f"Weight: {completeness['weight']}/{total} ({completeness['weight']/total*100:.1f}%)")

if __name__ == '__main__':
    main()
```

Run analysis:
```bash
python scripts/analyze_data_completeness.py
```

**Expected Output:**
```
=== FHIR Data Completeness Report ===

Total Records Analyzed: 665

Gestational Age: 598/665 (89.9%)
Blood Pressure: 612/665 (92.0%)
Fetal Heart Rate: 487/665 (73.2%)
Weight: 645/665 (97.0%)
```

#### Human Offline Tasks
- [ ] **Review Synthea configuration** - Ensure Synthea is generating Observation resources
- [ ] **Validate clinical accuracy** - Are extracted vital signs realistic for pregnancy?
- [ ] **Document LOINC codes** - Which observation codes are we targeting?
- [ ] **Generate additional health records** if needed to reach 90% completeness target

If Synthea isn't generating enough observations, human must:
1. Update `synthea.properties` to enable pregnancy observations
2. Re-generate health records:
```bash
cd synthea
java -jar synthea-with-dependencies.jar -s 12345 -p 1000 Massachusetts "Pregnancy Module"
```

#### Acceptance Criteria
- [x] ‚â•90% of interviews have pregnancy weeks populated
- [x] ‚â•80% have blood pressure data
- [x] ‚â•70% have fetal heart rate (if clinically appropriate for stage)
- [x] Clear documentation of which FHIR resources are parsed
- [x] Graceful handling of missing data (no crashes)

**Deliverable:** Enhanced `scripts/utils/fhir_semantic_extractor.py` with comprehensive vital signs extraction

---

### 1.3 Calibrate Anomaly Detection üü° P1 MEDIUM

**Current State:** 100% false positive rate (all interviews flagged as anomalous)
**Target State:** <10% false positive rate on normal data
**Priority:** P1 (Quality Assurance)
**Effort:** 20 hours

#### Problem Analysis
Anomaly detection thresholds are set statically without baseline data, causing every interview to be flagged.

**File:** `scripts/05_analyze_interviews.py` or `scripts/analyze_interviews.py`

#### Implementation Steps (Claude Code)

##### Step 1.3.1: Generate Baseline Metrics from 100-Interview Dataset
**Claude Code Instructions:**
1. Create `scripts/generate_baseline_metrics.py`:

```python
#!/usr/bin/env python3
"""
Generate baseline metrics for anomaly detection calibration.

This script:
1. Analyzes 100 diverse interviews
2. Calculates statistical distributions for key metrics
3. Determines appropriate thresholds (mean + 2-3 std dev)
4. Saves thresholds to config file

Metrics analyzed:
- Interview length (word count)
- Conversation turns
- Topic coverage
- Sentiment scores
- Cost per interview
- Response time
"""

import json
import statistics
from pathlib import Path
from typing import List, Dict, Any
import yaml

def load_interviews(interview_dir: Path, max_count: int = 100) -> List[Dict[str, Any]]:
    """Load up to max_count interviews."""
    interviews = []

    for interview_file in sorted(interview_dir.glob('*.json'))[:max_count]:
        with open(interview_file, 'r') as f:
            interviews.append(json.load(f))

    return interviews


def calculate_interview_metrics(interview: Dict[str, Any]) -> Dict[str, float]:
    """Calculate metrics for a single interview."""
    transcript = interview.get('transcript', [])

    total_words = sum(len(turn.get('text', '').split()) for turn in transcript)
    conversation_turns = len(transcript)

    # Topic coverage (count unique topics mentioned)
    topics = set()
    for turn in transcript:
        text = turn.get('text', '').lower()
        if 'pregnancy' in text:
            topics.add('pregnancy')
        if 'care' in text or 'doctor' in text:
            topics.add('prenatal_care')
        if 'health' in text:
            topics.add('health')
        # Add more topic detection...

    topic_coverage = len(topics)

    # Cost metrics
    cost = interview.get('cost_usd', 0.0)

    return {
        'total_words': total_words,
        'conversation_turns': conversation_turns,
        'topic_coverage': topic_coverage,
        'cost_usd': cost
    }


def calculate_thresholds(metrics_list: List[Dict[str, float]]) -> Dict[str, Dict[str, float]]:
    """Calculate statistical thresholds for anomaly detection."""

    thresholds = {}

    # For each metric, calculate mean, std, and thresholds
    metric_names = metrics_list[0].keys()

    for metric_name in metric_names:
        values = [m[metric_name] for m in metrics_list]

        mean = statistics.mean(values)
        stdev = statistics.stdev(values) if len(values) > 1 else 0

        # Thresholds:
        # - Warning: mean +/- 2 std dev
        # - Critical: mean +/- 3 std dev
        # - Also use percentiles for robustness

        values_sorted = sorted(values)
        p01 = values_sorted[int(len(values) * 0.01)]  # 1st percentile
        p99 = values_sorted[int(len(values) * 0.99)]  # 99th percentile

        thresholds[metric_name] = {
            'mean': mean,
            'std': stdev,
            'min_warning': max(mean - 2 * stdev, p01),
            'max_warning': min(mean + 2 * stdev, p99),
            'min_critical': max(mean - 3 * stdev, p01),
            'max_critical': min(mean + 3 * stdev, p99)
        }

    return thresholds


def main():
    interviews_dir = Path('data/interviews')

    if not interviews_dir.exists():
        print("ERROR: No interviews directory found. Generate interviews first.")
        return

    print("Loading interviews for baseline calculation...")
    interviews = load_interviews(interviews_dir, max_count=100)

    if len(interviews) < 20:
        print(f"WARNING: Only {len(interviews)} interviews found. Need at least 20 for reliable statistics.")
        print("Generate more interviews first.")
        return

    print(f"Analyzing {len(interviews)} interviews...")

    # Calculate metrics for all interviews
    metrics_list = [calculate_interview_metrics(interview) for interview in interviews]

    # Calculate thresholds
    thresholds = calculate_thresholds(metrics_list)

    # Save to config file
    config_file = Path('config/anomaly_thresholds.yaml')
    config_file.parent.mkdir(exist_ok=True)

    with open(config_file, 'w') as f:
        yaml.dump(thresholds, f, default_flow_style=False)

    print(f"\n‚úì Baseline thresholds calculated and saved to {config_file}")
    print("\nThresholds Summary:")
    for metric, values in thresholds.items():
        print(f"\n{metric}:")
        print(f"  Mean: {values['mean']:.2f} ¬± {values['std']:.2f}")
        print(f"  Warning range: [{values['min_warning']:.2f}, {values['max_warning']:.2f}]")
        print(f"  Critical range: [{values['min_critical']:.2f}, {values['max_critical']:.2f}]")


if __name__ == '__main__':
    main()
```

2. Run baseline generation:
```bash
# First, ensure you have 100+ interviews
python scripts/04_conduct_interviews.py --count 100

# Then generate baselines
python scripts/generate_baseline_metrics.py
```

##### Step 1.3.2: Implement Dynamic Threshold Adjustment
**File:** `scripts/analyze_interviews.py`

**Claude Code Instructions:**
1. Load thresholds from config instead of hardcoding:

```python
import yaml

def load_anomaly_thresholds() -> Dict[str, Dict[str, float]]:
    """Load anomaly detection thresholds from config."""
    threshold_file = Path('config/anomaly_thresholds.yaml')

    if not threshold_file.exists():
        logger.warning("Anomaly thresholds not calibrated. Using defaults.")
        return get_default_thresholds()

    with open(threshold_file, 'r') as f:
        return yaml.safe_load(f)


def get_default_thresholds() -> Dict[str, Dict[str, float]]:
    """Return conservative default thresholds if not calibrated."""
    return {
        'total_words': {
            'mean': 3000,
            'std': 1000,
            'min_warning': 1000,
            'max_warning': 8000,
            'min_critical': 500,
            'max_critical': 10000
        },
        'conversation_turns': {
            'mean': 34,
            'std': 10,
            'min_warning': 15,
            'max_warning': 60,
            'min_critical': 10,
            'max_critical': 80
        }
    }
```

2. Update anomaly detection function:

```python
def detect_anomalies(
    interview: Dict[str, Any],
    thresholds: Dict[str, Dict[str, float]]
) -> Dict[str, Any]:
    """
    Detect anomalies in interview based on calibrated thresholds.

    Returns:
        Dict with:
        - is_anomalous: bool
        - severity: "normal" | "warning" | "critical"
        - reasons: List[str] explaining why flagged
    """
    metrics = calculate_interview_metrics(interview)

    anomalies = []
    max_severity = "normal"

    for metric_name, value in metrics.items():
        if metric_name not in thresholds:
            continue

        t = thresholds[metric_name]

        # Check if outside critical range
        if value < t['min_critical'] or value > t['max_critical']:
            anomalies.append(f"{metric_name} ({value:.1f}) outside critical range [{t['min_critical']:.1f}, {t['max_critical']:.1f}]")
            max_severity = "critical"

        # Check if outside warning range
        elif value < t['min_warning'] or value > t['max_warning']:
            if max_severity != "critical":
                max_severity = "warning"
            anomalies.append(f"{metric_name} ({value:.1f}) outside warning range [{t['min_warning']:.1f}, {t['max_warning']:.1f}]")

    return {
        'is_anomalous': len(anomalies) > 0,
        'severity': max_severity,
        'reasons': anomalies,
        'metrics': metrics
    }
```

##### Step 1.3.3: Add Percentile-Based Outlier Detection
**Claude Code Instructions:**
Add alternative percentile-based detection:

```python
def detect_outliers_percentile(
    metrics: Dict[str, float],
    baseline_distributions: Dict[str, List[float]]
) -> bool:
    """
    Detect outliers using percentile method.

    An interview is an outlier if ANY metric is outside the 1st-99th percentile range.
    """
    for metric_name, value in metrics.items():
        if metric_name not in baseline_distributions:
            continue

        distribution = sorted(baseline_distributions[metric_name])
        p01 = distribution[int(len(distribution) * 0.01)]
        p99 = distribution[int(len(distribution) * 0.99)]

        if value < p01 or value > p99:
            return True

    return False
```

##### Step 1.3.4: Create Separate Thresholds for Different Interview Protocols
**Claude Code Instructions:**
When multiple protocols are implemented (Phase 2), create protocol-specific thresholds:

```python
def calculate_protocol_specific_thresholds(
    interviews: List[Dict[str, Any]]
) -> Dict[str, Dict[str, Dict[str, float]]]:
    """
    Calculate thresholds separately for each interview protocol.

    Returns:
        {
            'prenatal_care': {...thresholds...},
            'postpartum_care': {...thresholds...},
            ...
        }
    """
    interviews_by_protocol = {}

    for interview in interviews:
        protocol = interview.get('protocol_name', 'unknown')
        if protocol not in interviews_by_protocol:
            interviews_by_protocol[protocol] = []
        interviews_by_protocol[protocol].append(interview)

    thresholds_by_protocol = {}

    for protocol, protocol_interviews in interviews_by_protocol.items():
        metrics_list = [calculate_interview_metrics(i) for i in protocol_interviews]
        thresholds_by_protocol[protocol] = calculate_thresholds(metrics_list)

    return thresholds_by_protocol
```

##### Step 1.3.5: Add Flagging Severity Levels
**Claude Code Instructions:**
Create clear severity classification:

```python
class AnomalySeverity(Enum):
    NORMAL = "normal"
    INFO = "info"  # Slightly unusual but not concerning
    WARNING = "warning"  # Unusual, should review
    CRITICAL = "critical"  # Definitely problematic


def classify_anomaly_severity(
    metric_value: float,
    thresholds: Dict[str, float]
) -> AnomalySeverity:
    """Classify severity based on how far outside normal range."""

    if thresholds['min_critical'] <= metric_value <= thresholds['max_critical']:
        if thresholds['min_warning'] <= metric_value <= thresholds['max_warning']:
            return AnomalySeverity.NORMAL
        else:
            return AnomalySeverity.WARNING
    else:
        # Check how far outside critical range
        if metric_value < thresholds['min_critical']:
            distance = (thresholds['min_critical'] - metric_value) / thresholds['std']
        else:
            distance = (metric_value - thresholds['max_critical']) / thresholds['std']

        if distance > 5:  # More than 5 standard deviations away
            return AnomalySeverity.CRITICAL
        else:
            return AnomalySeverity.WARNING
```

#### Testing Instructions (Claude Code)
Create `tests/test_anomaly_detection.py`:

```python
import pytest
from scripts.analyze_interviews import detect_anomalies, load_anomaly_thresholds

def test_normal_interview_not_flagged():
    """Test that a normal interview is not flagged as anomalous."""

    normal_interview = {
        'transcript': [
            {'speaker': 'interviewer', 'text': 'How is your pregnancy going?'},
            {'speaker': 'participant', 'text': 'It is going well, thank you.'},
            # ... 30 more turns ...
        ],
        'cost_usd': 0.25,
        'total_words': 3200
    }

    thresholds = load_anomaly_thresholds()
    result = detect_anomalies(normal_interview, thresholds)

    assert result['severity'] == 'normal'
    assert not result['is_anomalous']


def test_too_short_interview_flagged():
    """Test that an unusually short interview is flagged."""

    short_interview = {
        'transcript': [
            {'speaker': 'interviewer', 'text': 'Hello'},
            {'speaker': 'participant', 'text': 'Hi'},
        ],
        'cost_usd': 0.05,
        'total_words': 50
    }

    thresholds = load_anomaly_thresholds()
    result = detect_anomalies(short_interview, thresholds)

    assert result['is_anomalous']
    assert 'total_words' in str(result['reasons'])


def test_false_positive_rate_under_10_percent():
    """Test that false positive rate is under 10% on normal dataset."""

    # Load 100 known-good interviews
    interviews = load_interviews(Path('data/interviews'), max_count=100)
    thresholds = load_anomaly_thresholds()

    false_positives = 0

    for interview in interviews:
        result = detect_anomalies(interview, thresholds)
        if result['is_anomalous']:
            false_positives += 1

    false_positive_rate = false_positives / len(interviews)

    print(f"\nFalse Positive Rate: {false_positive_rate * 100:.1f}%")
    print(f"Flagged: {false_positives}/{len(interviews)}")

    assert false_positive_rate < 0.10, f"False positive rate {false_positive_rate*100:.1f}% exceeds 10% threshold"
```

Run tests:
```bash
pytest tests/test_anomaly_detection.py -v
```

#### Human Offline Tasks
- [ ] **Manually review** 10-20 interviews flagged as anomalies to verify they are genuinely unusual
- [ ] **Manually review** 10-20 interviews NOT flagged to verify they are genuinely normal
- [ ] **Adjust thresholds** if false positive/negative rates are too high
- [ ] **Document** what constitutes a "good" vs "bad" interview for future reference

#### Acceptance Criteria
- [x] Run on 100 diverse interviews
- [x] Flag ‚â§10% as anomalies (not 100%)
- [x] True anomalies (manually identified) detected at >90% rate
- [x] Clear explanation of why each interview is flagged
- [x] Protocol-specific thresholds when multiple protocols exist

**Deliverable:** Recalibrated `scripts/analyze_interviews.py` with validated thresholds

---

### 1.4 Add Automated Testing üü° P1 HIGH

**Current State:** Zero automated tests
**Target State:** ‚â•60% code coverage with pytest
**Priority:** P1 (Code Quality)
**Effort:** 25 hours

#### Implementation Steps (Claude Code)

##### Step 1.4.1: Create Test Directory Structure
**Claude Code Instructions:**
```bash
# Create comprehensive test directory
mkdir -p tests/{unit,integration,fixtures}
touch tests/unit/__init__.py
touch tests/integration/__init__.py
touch tests/fixtures/__init__.py
```

##### Step 1.4.2: Add Unit Tests for Semantic Tree Builder
**File:** `tests/unit/test_semantic_tree.py`

**Claude Code Instructions:**
Create comprehensive unit tests:

```python
import pytest
from scripts.utils.semantic_tree import (
    PersonaSemanticTree,
    DemographicsNode,
    SocioeconomicNode,
    HealthProfileNode,
    BehavioralNode,
    PsychosocialNode,
    PregnancyIntentionsNode,
    calculate_semantic_tree_similarity
)

class TestDemographicsNode:
    def test_valid_demographics(self):
        """Test creating valid demographics node."""
        demo = DemographicsNode(
            age=28,
            gender="female",
            location_type="urban",
            ethnicity="Hispanic",
            language_primary="Spanish"
        )

        assert demo.age == 28
        assert demo.validate()

    def test_invalid_age_warns(self, caplog):
        """Test that unusual ages trigger warnings."""
        demo = DemographicsNode(
            age=5,  # Too young for pregnancy study
            gender="female",
            location_type="urban"
        )

        demo.validate()
        assert "outside typical range" in caplog.text.lower()


class TestSemanticTreeSimilarity:
    def test_identical_trees_score_1(self):
        """Test that identical trees have similarity = 1.0."""
        persona_tree = create_test_persona_tree()  # Helper function
        record_tree = create_identical_record_tree()  # Helper function

        similarity, components = calculate_semantic_tree_similarity(
            persona_tree,
            record_tree
        )

        assert similarity >= 0.95  # Allow small floating point differences

    def test_completely_different_trees_score_low(self):
        """Test that very different trees have low similarity."""
        persona_tree = create_young_healthy_persona()
        record_tree = create_elderly_sick_record()

        similarity, components = calculate_semantic_tree_similarity(
            persona_tree,
            record_tree
        )

        assert similarity < 0.3  # Should be low


# Helper fixtures
@pytest.fixture
def sample_persona_tree():
    """Create a sample persona tree for testing."""
    return PersonaSemanticTree(
        persona_id=1,
        demographics=DemographicsNode(
            age=30,
            gender="female",
            location_type="suburban"
        ),
        socioeconomic=SocioeconomicNode(
            education_level="bachelors",
            income_bracket="middle",
            occupation_category="professional",
            employment_status="employed",
            insurance_status="insured"
        ),
        health_profile=HealthProfileNode(
            health_consciousness=4,
            healthcare_access=4,
            pregnancy_readiness=4
        ),
        behavioral=BehavioralNode(
            physical_activity_level=3,
            nutrition_awareness=4,
            smoking_status="never",
            alcohol_consumption="occasional",
            substance_use="none",
            sleep_quality=3
        ),
        psychosocial=PsychosocialNode(
            mental_health_status=4,
            stress_level=2,
            social_support=4,
            marital_status="married",
            relationship_stability=4,
            financial_stress=2,
            family_planning_attitudes="wants_children"
        ),
        pregnancy_intentions=PregnancyIntentionsNode(
            gravida=1,
            para=0,
            trying_duration=6
        )
    )
```

##### Step 1.4.3: Add Unit Tests for FHIR Parser
**File:** `tests/unit/test_fhir_parser.py`

**Claude Code Instructions:**
```python
import pytest
from scripts.utils.fhir_semantic_extractor import (
    build_semantic_tree_from_fhir,
    extract_vital_signs_from_observations,
    calculate_comorbidity_index
)

class TestFHIRParsing:
    def test_empty_fhir_bundle_returns_minimal_tree(self):
        """Test that empty FHIR data returns minimal semantic tree."""
        fhir_data = {'entry': []}

        tree = build_semantic_tree_from_fhir(fhir_data, "test-patient", 30)

        assert tree.patient_id == "test-patient"
        assert tree.age == 30
        assert len(tree.conditions) == 0

    def test_fhir_with_conditions_parsed_correctly(self):
        """Test that conditions are extracted from FHIR."""
        fhir_data = {
            'entry': [
                {
                    'resource': {
                        'resourceType': 'Condition',
                        'code': {
                            'coding': [{
                                'code': '72892002',
                                'display': 'Normal pregnancy'
                            }]
                        },
                        'onsetDateTime': '2025-01-01'
                    }
                }
            ]
        }

        tree = build_semantic_tree_from_fhir(fhir_data, "test-patient", 30)

        assert len(tree.conditions) == 1
        assert tree.conditions[0].display == 'Normal pregnancy'

    def test_missing_fields_dont_crash(self):
        """Test that missing FHIR fields don't cause crashes."""
        fhir_data = {
            'entry': [
                {
                    'resource': {
                        'resourceType': 'Condition',
                        'code': {}  # Missing coding
                    }
                }
            ]
        }

        # Should not raise exception
        tree = build_semantic_tree_from_fhir(fhir_data, "test-patient", 30)
        assert tree is not None


class TestVitalSignsExtraction:
    def test_extract_gestational_age(self):
        """Test gestational age extraction."""
        observations = [
            {
                'type': 'gestational_age',
                'value': 20.5,
                'unit': 'weeks',
                'date': '2025-01-15'
            }
        ]

        vitals = extract_vital_signs_from_observations(observations)

        assert vitals['gestational_age_weeks'] == 20.5

    def test_most_recent_observation_used(self):
        """Test that most recent observation is preferred."""
        observations = [
            {
                'type': 'gestational_age',
                'value': 18.0,
                'unit': 'weeks',
                'date': '2025-01-01'
            },
            {
                'type': 'gestational_age',
                'value': 22.0,
                'unit': 'weeks',
                'date': '2025-01-15'  # More recent
            }
        ]

        vitals = extract_vital_signs_from_observations(observations)

        assert vitals['gestational_age_weeks'] == 22.0


class TestComorbidityIndex:
    def test_no_conditions_zero_index(self):
        """Test that no conditions results in zero comorbidity index."""
        conditions = []
        index = calculate_comorbidity_index(conditions)
        assert index == 0.0

    def test_chronic_conditions_increase_index(self):
        """Test that chronic conditions increase comorbidity index."""
        conditions = [
            {'code': '73211009', 'display': 'Diabetes mellitus'},  # Chronic
            {'code': '38341003', 'display': 'Hypertension'},  # Chronic
        ]

        index = calculate_comorbidity_index(conditions)
        assert index > 0.3

    def test_index_bounded_0_to_1(self):
        """Test that comorbidity index is always 0.0-1.0."""
        # Test with many severe conditions
        conditions = [
            {'code': '73211009'} for _ in range(20)  # 20 chronic conditions
        ]

        index = calculate_comorbidity_index(conditions)
        assert 0.0 <= index <= 1.0
```

##### Step 1.4.4: Add Unit Tests for Matching Algorithm
**File:** `tests/unit/test_matching.py`

**Claude Code Instructions:**
```python
import pytest
import numpy as np
from scripts.utils.semantic_matcher import calculate_semantic_matching_score
from scripts.utils.matching_algorithm import calculate_age_compatibility

class TestAgeCompatibility:
    def test_exact_age_match_score_1(self):
        """Test that exact age match scores 1.0."""
        score = calculate_age_compatibility(30, 30)
        assert score == 1.0

    def test_age_within_tolerance_high_score(self):
        """Test that ages within tolerance get high scores."""
        score = calculate_age_compatibility(30, 32, tolerance=2)
        assert score >= 0.85

    def test_large_age_difference_low_score(self):
        """Test that large age differences get low scores."""
        score = calculate_age_compatibility(20, 45, tolerance=2)
        assert score < 0.3


class TestSemanticMatching:
    def test_semantic_score_in_range(self):
        """Test that semantic scores are always 0.0-1.0."""
        persona = create_test_persona()
        record = create_test_record()

        score = calculate_semantic_matching_score(persona, record)

        assert 0.0 <= score <= 1.0

    def test_high_match_quality_scores_high(self):
        """Test that well-matched pairs score high."""
        persona = create_persona_age_30_healthy()
        record = create_record_age_30_healthy()

        score = calculate_semantic_matching_score(persona, record)

        assert score >= 0.7
```

##### Step 1.4.5: Add Integration Tests for Full Pipeline
**File:** `tests/integration/test_full_pipeline.py`

**Claude Code Instructions:**
```python
import pytest
from pathlib import Path
import json

class TestFullPipeline:
    def test_persona_generation_creates_valid_files(self, tmp_path):
        """Test that persona generation creates valid JSON files."""
        # Use tmp_path for test isolation
        output_dir = tmp_path / "personas"

        # Run persona generation (with mocked AI client)
        # ... implementation ...

        assert output_dir.exists()
        persona_files = list(output_dir.glob("*.json"))
        assert len(persona_files) > 0

        # Validate first persona
        with open(persona_files[0], 'r') as f:
            persona = json.load(f)

        assert 'persona_id' in persona
        assert 'age' in persona

    def test_matching_creates_matched_pairs(self, tmp_path):
        """Test that matching creates valid matched pair files."""
        # Set up test data
        personas_dir = tmp_path / "personas"
        records_dir = tmp_path / "health_records"
        output_dir = tmp_path / "matched"

        # Create test personas and records
        # ... setup ...

        # Run matching
        # ... implementation ...

        matched_files = list(output_dir.glob("*.json"))
        assert len(matched_files) > 0

        # Validate matched pair structure
        with open(matched_files[0], 'r') as f:
            matched = json.load(f)

        assert 'persona' in matched
        assert 'health_record' in matched
        assert 'match_score' in matched

    def test_interview_execution_end_to_end(self, tmp_path):
        """Test complete interview execution from matched pair to transcript."""
        # This is a longer integration test
        # Use mocked AI client to avoid API costs

        # ... implementation ...

        pass  # Implement based on your pipeline
```

##### Step 1.4.6: Add Fixture Data for Reproducible Testing
**File:** `tests/fixtures/sample_data.py`

**Claude Code Instructions:**
```python
"""
Test fixtures for reproducible testing.

Provides sample personas, health records, and FHIR bundles for testing.
"""

def get_sample_fhir_bundle():
    """Return a sample FHIR bundle with complete data."""
    return {
        'resourceType': 'Bundle',
        'entry': [
            {
                'resource': {
                    'resourceType': 'Patient',
                    'id': 'test-patient-001',
                    'birthDate': '1995-06-15',
                    'gender': 'female'
                }
            },
            {
                'resource': {
                    'resourceType': 'Condition',
                    'code': {
                        'coding': [{
                            'system': 'http://snomed.info/sct',
                            'code': '72892002',
                            'display': 'Normal pregnancy'
                        }]
                    },
                    'onsetDateTime': '2024-10-01'
                }
            },
            {
                'resource': {
                    'resourceType': 'Observation',
                    'code': {
                        'coding': [{
                            'code': '18185-9',
                            'display': 'Gestational age'
                        }]
                    },
                    'valueQuantity': {
                        'value': 24.0,
                        'unit': 'weeks'
                    },
                    'effectiveDateTime': '2025-01-15'
                }
            }
        ]
    }


def get_sample_persona():
    """Return a sample persona for testing."""
    return {
        'persona_id': 1,
        'age': 30,
        'occupation': 'Software Engineer',
        'education': 'bachelors',
        'income': '$75,000',
        'location': 'Boston, MA',
        'description': 'Sarah is a 30-year-old software engineer...'
    }


def get_sample_matched_pair():
    """Return a sample matched persona-record pair."""
    return {
        'persona': get_sample_persona(),
        'health_record': {
            'patient_id': 'test-patient-001',
            'age': 30,
            'conditions': ['Normal pregnancy'],
            'medications': []
        },
        'match_score': 0.92,
        'match_components': {
            'age_compatibility': 1.0,
            'semantic_similarity': 0.88,
            'socioeconomic_match': 0.90
        }
    }
```

##### Step 1.4.7: Implement CI/CD with GitHub Actions (Optional)
**File:** `.github/workflows/tests.yml`

**Claude Code Instructions:**
```yaml
name: Run Tests

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]

jobs:
  test:
    runs-on: ubuntu-latest

    strategy:
      matrix:
        python-version: ['3.9', '3.10', '3.11']

    steps:
    - uses: actions/checkout@v3

    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest pytest-cov

    - name: Run tests with coverage
      run: |
        pytest tests/ --cov=scripts --cov-report=xml --cov-report=term

    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        fail_ci_if_error: false
```

##### Step 1.4.8: Add pytest Configuration
**File:** `pytest.ini`

**Claude Code Instructions:**
```ini
[pytest]
testpaths = tests
python_files = test_*.py
python_classes = Test*
python_functions = test_*

# Coverage settings
addopts =
    --verbose
    --cov=scripts
    --cov-report=html
    --cov-report=term-missing
    --cov-fail-under=60

# Markers for categorizing tests
markers =
    unit: Unit tests
    integration: Integration tests
    slow: Slow tests (e.g., requiring API calls)
    requires_data: Tests requiring generated data
```

##### Step 1.4.9: Add Test Documentation
**File:** `tests/README.md`

**Claude Code Instructions:**
```markdown
# Gravidas Pipeline - Test Suite

## Overview

Comprehensive test suite ensuring code quality and correctness across the Gravidas pipeline.

## Test Structure

```
tests/
‚îú‚îÄ‚îÄ unit/                    # Unit tests for individual modules
‚îÇ   ‚îú‚îÄ‚îÄ test_semantic_tree.py
‚îÇ   ‚îú‚îÄ‚îÄ test_fhir_parser.py
‚îÇ   ‚îî‚îÄ‚îÄ test_matching.py
‚îú‚îÄ‚îÄ integration/             # Integration tests for full pipeline
‚îÇ   ‚îî‚îÄ‚îÄ test_full_pipeline.py
‚îú‚îÄ‚îÄ fixtures/                # Test data and fixtures
‚îÇ   ‚îî‚îÄ‚îÄ sample_data.py
‚îî‚îÄ‚îÄ conftest.py             # Pytest configuration and shared fixtures
```

## Running Tests

### Run all tests
```bash
pytest
```

### Run only unit tests
```bash
pytest tests/unit/
```

### Run with coverage report
```bash
pytest --cov=scripts --cov-report=html
```

### Run specific test file
```bash
pytest tests/unit/test_semantic_tree.py -v
```

### Run tests matching pattern
```bash
pytest -k "test_fhir" -v
```

## Coverage Goals

- **Overall:** ‚â•60% code coverage
- **Core modules:** ‚â•80% coverage
  - `scripts/utils/semantic_tree.py`
  - `scripts/utils/fhir_semantic_extractor.py`
  - `scripts/utils/semantic_matcher.py`

## Writing Tests

### Unit Test Template

```python
import pytest
from scripts.utils.your_module import your_function

class TestYourFunction:
    def test_normal_case(self):
        """Test normal/expected behavior."""
        result = your_function(input_data)
        assert result == expected_output

    def test_edge_case(self):
        """Test edge cases."""
        result = your_function(edge_case_input)
        assert result is not None

    def test_error_handling(self):
        """Test error handling."""
        with pytest.raises(ValueError):
            your_function(invalid_input)
```

### Using Fixtures

```python
@pytest.fixture
def sample_persona():
    """Fixture providing sample persona for tests."""
    return {
        'persona_id': 1,
        'age': 30,
        # ... more fields
    }

def test_with_fixture(sample_persona):
    """Test using the fixture."""
    assert sample_persona['age'] == 30
```

## Test Markers

Use markers to categorize tests:

```python
@pytest.mark.unit
def test_unit_function():
    pass

@pytest.mark.integration
def test_full_pipeline():
    pass

@pytest.mark.slow
def test_large_dataset():
    pass
```

Run specific markers:
```bash
pytest -m unit  # Run only unit tests
pytest -m "not slow"  # Skip slow tests
```

## Continuous Integration

Tests run automatically on:
- Every push to `main` or `develop`
- Every pull request

See `.github/workflows/tests.yml` for CI configuration.

## Troubleshooting

### Tests fail due to missing data
Run data generation first:
```bash
python scripts/01b_generate_personas.py --count 10
python scripts/02_generate_health_records.py --count 10
```

### Import errors
Ensure you're running from repository root and have installed dependencies:
```bash
pip install -r requirements.txt
```

### Coverage not reaching 60%
Focus on testing:
1. Core utility modules first
2. Critical path code
3. Error handling paths
```
```

#### Testing Instructions (Claude Code)
Run complete test suite:
```bash
# Install test dependencies
pip install pytest pytest-cov pytest-mock

# Run all tests
pytest -v

# Run with coverage
pytest --cov=scripts --cov-report=html --cov-report=term

# View coverage report
open htmlcov/index.html  # On macOS
xdg-open htmlcov/index.html  # On Linux
```

#### Human Offline Tasks
- [ ] **Review test coverage report** - Identify gaps in coverage
- [ ] **Manually test** edge cases that are hard to automate
- [ ] **Code review** test code for quality and maintainability
- [ ] **Document** any manual testing procedures that can't be automated

#### Acceptance Criteria
- [x] ‚â•60% code coverage on core modules
- [x] All tests pass on Python 3.11+
- [x] Tests run in <2 minutes
- [x] Documented test strategy in `tests/README.md`
- [x] CI/CD pipeline configured (optional but recommended)

**Deliverable:** `tests/` directory with comprehensive test suite achieving ‚â•60% coverage

---

## Phase 2: Enhanced Capabilities (Weeks 5-8)

*(To be continued in next section due to length...)*

---

## Summary of Phase 1

**Total Effort:** 115 hours over 4 weeks
**Critical Path:** 1.1 ‚Üí 1.2 ‚Üí 1.3 ‚Üí 1.4

**Key Deliverables:**
1. ‚úÖ Semantic tree generation with ‚â•95% success rate
2. ‚úÖ Complete FHIR data extraction with ‚â•90% completeness
3. ‚úÖ Calibrated anomaly detection with <10% false positive rate
4. ‚úÖ Test suite with ‚â•60% code coverage

**Phase 1 Acceptance Gate:**
All four critical fixes must be complete and tested before proceeding to Phase 2.

---

*Document continues with Phase 2, Phase 3, Phase 4...*
*Would you like me to continue with the remaining phases in detail?*
